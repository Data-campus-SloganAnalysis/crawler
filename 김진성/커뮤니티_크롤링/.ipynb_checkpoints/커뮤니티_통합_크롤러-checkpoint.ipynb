{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9d4e041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "\n",
    "driver_path = './chromedriver.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b08fe33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    cleaned_text = re.sub('[a-zA-Z]', '', text)\n",
    "    cleaned_text = re.sub('[\\{\\}\\[\\]\\/?.,;:|\\)*~`!^\\-_+<>@\\#$%&\\\\\\=\\(\\'\\\"]',\n",
    "                          '', cleaned_text)\n",
    "    cleaned_text = ' '.join(cleaned_text.split())\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e8b7b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppomppu_crawler(query,driver_path,page):\n",
    "    driver = webdriver.Chrome(executable_path=driver_path)\n",
    "    text = []\n",
    "    \n",
    "    for i in range(1,page+1):\n",
    "        post_links = [] \n",
    "        \n",
    "        base_url = 'https://www.ppomppu.co.kr/'\n",
    "        parm = 'search_bbs.php?page_size=50&bbs_cate=2&keyword=' + query + '&order_type=date&search_type=sub_memo&page_no='+str(i)\n",
    "        url = base_url + parm\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "        \n",
    "        for j in range(1,51):\n",
    "            if soup.select('body > div > div.contents > div.container > div > form > div > div:nth-of-type('+str(j)+') > div > p.desc > span:nth-of-type(1)')[0].text != '[뽐뿌게시판]':\n",
    "                try:\n",
    "                    if '예고' not in soup.select('body > div > div.contents > div.container > div > form > div > div:nth-of-type('+str(j)+') > div > span > a')[0].text:\n",
    "                        if '유안타' not in soup.select('body > div > div.contents > div.container > div > form > div > div:nth-of-type('+str(j)+') > div > span > a')[0].text:\n",
    "                            post_links.append(soup.select('body > div > div.contents > div.container > div > form > div > div:nth-of-type('+str(j)+') > div > span > a'))\n",
    "                except:\n",
    "                    print('error link')\n",
    "            \n",
    "        for k in range(len(post_links)):\n",
    "            try:\n",
    "                base_url = 'https://www.ppomppu.co.kr/'\n",
    "                link_url = base_url + post_links[k][0].get('href')\n",
    "                driver.get(link_url)\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                temp = soup.find_all('td',class_='board-contents')\n",
    "                text.append(clean_text(temp[0].text))\n",
    "            except:\n",
    "                print('error link : ',post_links[k])\n",
    "\n",
    "    driver.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7786c56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmkorea_document(query,driver_path,page):\n",
    "    driver = webdriver.Chrome(executable_path=driver_path)\n",
    "    data_list=[]\n",
    "\n",
    "    for i in range(1,page+1):\n",
    "        base_url = 'https://www.fmkorea.com/?act=IS&is_keyword='\n",
    "        parm = query+'&mid=home&where=document&page='+str(i)\n",
    "        url = base_url + parm\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "        for j in range(1,11):\n",
    "            regex=\"\\[[^]]*\\]\"\n",
    "            title=soup.select('#content > div > ul.searchResult > li:nth-of-type('+str(j)+') > dl > dt > a')\n",
    "            title=title[0].text\n",
    "            title=re.sub(regex,'',str(title))\n",
    "            title=clean_text(title)\n",
    "            \n",
    "            contents=soup.select('#content > div > ul.searchResult > li:nth-of-type('+str(j)+') > dl > dd')\n",
    "            contents=clean_text(contents[0].text)\n",
    "            data_list.append(title+contents)\n",
    "    driver.close()\n",
    "\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fec17f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def instiz(query,driver_path,page):\n",
    "    driver = webdriver.Chrome(executable_path=driver_path)\n",
    "    data_list=[]\n",
    "    \n",
    "    for i in range(1,page+1):\n",
    "        base_url = 'https://www.instiz.net/popup_search.htm#gsc.tab=0&gsc.q='\n",
    "        parm = query+'&gsc.page='+str(i)\n",
    "        url = base_url + parm\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "        title = ''\n",
    "        content = ''\n",
    "        comment = ''\n",
    "        for j in range(1,11):\n",
    "            try:\n",
    "                content_url=soup.select('#___gcse_0 > div > div > div > div.gsc-wrapper > div.gsc-resultsbox-visible > div.gsc-resultsRoot.gsc-tabData.gsc-tabdActive > div > div.gsc-expansionArea > div:nth-of-type('+str(j)+') > div.gs-webResult.gs-result > div.gsc-thumbnail-inside > div > a')[0].get('href')\n",
    "                driver.get(str(content_url))\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                try:\n",
    "                    title=soup.select('#nowsubject > a')[0].text\n",
    "                    content=soup.select('#memo_content_1')[0].text\n",
    "                    content=content.replace('  에 게시된 글입니다','')\n",
    "                    content=content.replace('← 빈공간을 더블탭 해보세요 →','')\n",
    "                    comment=soup.select('#ajax_table > tbody')[0].text\n",
    "                    comment=clean_text(comment)\n",
    "                    #임의로 전처리...\n",
    "                    comment=comment.replace('•••','')\n",
    "                    comment=comment.replace('답글 스크랩 신고','')\n",
    "                except:\n",
    "                    print(\"\")\n",
    "                driver.get(url)\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                data_list.append(title+content+comment)\n",
    "            except:\n",
    "                print('')\n",
    "    driver.close()\n",
    "\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41a78c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nate_pann_crawler(query,driver_path,page):\n",
    "    driver = webdriver.Chrome(executable_path=driver_path)\n",
    "    text = []\n",
    "    \n",
    "    for i in range(1,page+1):\n",
    "        post_links = [] \n",
    "        \n",
    "        base_url = 'https://pann.nate.com'\n",
    "        parm = '/search/talk?q=' + query + '&sort=DD&page='+str(i)\n",
    "        url = base_url + parm\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "        \n",
    "        for j in range(1,11):\n",
    "            if '주식뉴스모음' not in soup.select('#container > div.content.sub > div.srcharea > div.srch_list.section > ul > li:nth-of-type('+str(j)+') > div.tit > a')[0].text:\n",
    "                post_links.append(soup.select('#container > div.content.sub > div.srcharea > div.srch_list.section > ul > li:nth-of-type('+str(j)+') > div.tit > a'))\n",
    "    \n",
    "        for k in range(len(post_links)):\n",
    "            try:\n",
    "                link_url = base_url + post_links[k][0].get('href')\n",
    "                driver.get(link_url)\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                temp = soup.select('#contentArea')[0].text\n",
    "                text.append(clean_text(temp))\n",
    "            except:\n",
    "                print('error link : ',post_links[k])\n",
    "\n",
    "    driver.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "592d38dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dc(query, driver_path, page):\n",
    "    \n",
    "    driver = webdriver.Chrome(executable_path=driver_path)\n",
    "    \n",
    "    text= []\n",
    "    \n",
    "    for i in range(1, page+1):\n",
    "        post_links = []\n",
    "        \n",
    "        base_url = 'https://search.dcinside.com/post/q/.'\n",
    "        \n",
    "        url = base_url + query + '/p/'+str(i)\n",
    "       \n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        for j in range(1,26):\n",
    "            post_links.append(soup.select('#container > div > section.center_content > div.inner > div.integrate_cont.sch_result.result_all > ul > li:nth-of-type('+str(j)+') > a'))\n",
    "            \n",
    "            \n",
    "        for k in range(len(post_links)):\n",
    "            \n",
    "            try:\n",
    "                link_url =  post_links[k][0].get('href')\n",
    "                driver.get(link_url)\n",
    "                soup = BeautifulSoup(driver.page_source , 'html.parser')\n",
    "                temp = soup.find_all('div',class_='write_div')[0].text\n",
    "                \n",
    "                text.append(clean_text(temp))\n",
    "            except:\n",
    "                \n",
    "                print('error_link: ', post_links[k])\n",
    "    driver.close()\n",
    "    return text\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef9ca3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ruli(query, driver_path, page):\n",
    "    driver = webdriver.Chrome(executable_path=driver_path)\n",
    "    text= []\n",
    "    \n",
    "    for i in range(1, page+1):\n",
    "        base_url = 'https://bbs.ruliweb.com/search?q='\n",
    "        url = base_url + query + '&page='+str(i) +'&c_page='+str(i)+'#comment_search&gsc.tab=0&gsc.q='+query+'&gsc.page=1'\n",
    "        driver.get(url)\n",
    "        driver.refresh()\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        for j in range(1,16):\n",
    "            comment=soup.select('#comment_search > div > ul > li:nth-of-type('+str(j)+') > div > div > a.title.text_over')\n",
    "            comment=comment[0].text\n",
    "            comment=clean_text(comment)\n",
    "            text.append(comment)\n",
    "            title = soup.select('#board_search > div > ul > li:nth-of-type('+str(j)+') > div > div > a.title.text_over')\n",
    "            title= title[0].text\n",
    "            tilte= clean_text(title) #게시물에 신문기사가 너무 많아서 우선 게시물의 제목만 가져왔습니다\n",
    "            text.append(title)\n",
    "    driver.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae267fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error link\n",
      "error link\n",
      "error link\n",
      "error link\n",
      "error link\n",
      "error link\n",
      "error link\n",
      "error link\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "error link :  [<a class=\"subject\" href=\"/talk/360977131\" title=\"[방탈죄송] 용인 사육농장에서 곰이 탈출 한 이유\">[방탈죄송] 용인 사육농장에서 곰이 탈출 한 이유</a>]\n",
      "error link :  [<a class=\"subject\" href=\"/talk/360882337\" title=\"내가 세상에게 하고 싶은 말\">내가 세상에게 하고 싶은 말</a>]\n",
      "error_link:  []\n",
      "error_link:  []\n",
      "error_link:  []\n",
      "error_link:  []\n",
      "error_link:  []\n",
      "error_link:  []\n",
      "error_link:  []\n",
      "error_link:  []\n",
      "error_link:  []\n"
     ]
    }
   ],
   "source": [
    "text = []\n",
    "\n",
    "key_word = '친환경'\n",
    "page = 20\n",
    "\n",
    "text.extend(ppomppu_crawler(key_word,driver_path,page))\n",
    "text.extend(fmkorea_document(key_word,driver_path,page))\n",
    "text.extend(instiz(key_word,driver_path,page))\n",
    "text.extend(nate_pann_crawler(key_word,driver_path,page))\n",
    "text.extend(dc(key_word,driver_path,page))\n",
    "text.extend(ruli(key_word,driver_path,page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2909810d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame(text,columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09b8c6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_index = df[df['text']==\"\"].index\n",
    "df.drop(drop_index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14345d53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# noun_list = []\n",
    "\n",
    "# for i in df['text']:\n",
    "#     noun_list.extend(tokenizer.nouns(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73482a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# import operator\n",
    "\n",
    "# counter = Counter(noun_list)\n",
    "# sort_nouns = sorted(counter.items(), key=operator.itemgetter(1),reverse=True)\n",
    "\n",
    "# f = open(\"sort_nouns.txt\", 'w', -1, \"utf-8\")\n",
    "\n",
    "# for i in sort_nouns:\n",
    "#     f.write(str(i)+'\\n')\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a06326",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from eunjeon import Mecab\n",
    "# tokenizer = Mecab()\n",
    "# pos_temp = {}\n",
    "# pos_list = []\n",
    "\n",
    "# for i in df['text']:\n",
    "#     temp_str = ''\n",
    "#     try:\n",
    "#         for j in range(len(tokenizer.pos(i))):\n",
    "#             pos_temp[tokenizer.pos(i)[j][1]]+=1\n",
    "#             temp_str+=tokenizer.pos(i)[j][1]+\"/\"\n",
    "#     except:\n",
    "#         for j in range(len(tokenizer.pos(i))):\n",
    "#             pos_temp[tokenizer.pos(i)[j][1]]=1\n",
    "#             temp_str+=tokenizer.pos(i)[j][1]+\"/\"\n",
    "#     pos_list.append(temp_str)\n",
    "\n",
    "# df['pos_list'] = pos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1031bda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('커뮤니티_크롤링_모음_친환경.csv',encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
