{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 크롤러 환경설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"headless\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각자 chromedriver.exe 경로 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_path = 'C:/Users/chaeh/chromedriver.exe'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특수문자 및 영어등 텍스트 전처리 함수 -> clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    cleaned_text = re.sub('[a-zA-Z]', '', text)\n",
    "    cleaned_text = re.sub('[\\{\\}\\[\\]\\/?.,;:|\\)*~`!^\\-_+<>@\\#$%&\\\\\\=\\(\\'\\\"]',\n",
    "                          '', cleaned_text)\n",
    "    cleaned_text = ' '.join(cleaned_text.split())\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 경향신문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biz_khan(query,driver_path,page):\n",
    "    head_list = []\n",
    "    data_list = []\n",
    "    current_page = 1 \n",
    "    last_page =(int(page)-1)*10+1\n",
    "    domain = '경향신문'\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options) \n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1032&nso=&is_sug_officeid=0' \n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "        for url in url_list:\n",
    "            driver = webdriver.Chrome(executable_path=driver_path, options=options)  # for Windows\n",
    "            driver.get(url)\n",
    "            time.sleep(2)\n",
    "            # try문 안에서 각자 언론사에 맞게 수정\n",
    "            try:\n",
    "                soup.find('h1',id='articleTtitle')\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(soup.find('h1',id='articleTtitle').text)\n",
    "                tags = soup.select_one('#container > div.main_container > div.art_cont > div.art_body').find_all('p')\n",
    "                data = [clean_text(x.text) for x in tags]\n",
    "                data_list.append(' '.join(data))\n",
    "            except:\n",
    "                try:\n",
    "                    soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                    head_list.append(soup.find('h1',id='article_title').text)\n",
    "                    tags = soup.select_one('#articleBody').find_all('p')\n",
    "                    data = [clean_text(x.text) for x in tags]\n",
    "                    data_list.append(' '.join(data))\n",
    "                except:\n",
    "                    print(url)\n",
    "                    print('None_type')\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "\n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 국민일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmib(query,driver_path,page):\n",
    "    domain = '국민일보'\n",
    "    driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    data_list = []\n",
    "    head_list = []\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1005&nso=so%3Ar%2Cp%3Aall&is_sug_officeid=0' \n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        #print(url)\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "        \n",
    "        for url in url_list:\n",
    "            driver = webdriver.Chrome(executable_path=driver_path,options=options)  # for Windows\n",
    "            time.sleep(2)\n",
    "            driver.get(url)\n",
    "            time.sleep(2)\n",
    "            # try문 안에서 각자 언론사에 맞게 수정\n",
    "            try:\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(soup.select_one('#sub > div.sub_header > div > div.nwsti > h3').text)\n",
    "                tags = soup.select_one(\"#articleBody\").text\n",
    "                data=tags.strip()\n",
    "                data=data.strip('\\t')\n",
    "                data=clean_text(data)\n",
    "                data_list.append(data)\n",
    "            except:\n",
    "                print(url)\n",
    "                print('None_type')\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "             \n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 내일신문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naeil(query,driver_path,page):\n",
    "    domain = '내일신문'\n",
    "    driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    data_list = []\n",
    "    head_list = []\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2312&nso=&is_sug_officeid=0' \n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        #print(url)\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "        for url in url_list:\n",
    "            driver = webdriver.Chrome(executable_path=driver_path,options=options)  # for Windows\n",
    "            time.sleep(2)\n",
    "            driver.get(url)\n",
    "            time.sleep(2)\n",
    "            # try문 안에서 각자 언론사에 맞게 수정\n",
    "            try:\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(soup.select_one('#contentArea > div.caL2 > div > div.articleArea > h3').text)\n",
    "                tags = soup.select_one(\"#contents\").text\n",
    "                data=tags.strip()\n",
    "                data=data.strip('\\t')\n",
    "                data=clean_text(data)\n",
    "                data_list.append(data)\n",
    "            except:\n",
    "                print(url)\n",
    "                print('None_type')\n",
    "            driver.close()\n",
    "        current_page+=10  \n",
    "        \n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 동아일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def donga(query,driver_path, page):\n",
    "    domain = '동아일보'\n",
    "    driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "    \n",
    "    current_page =1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    data_list =[]\n",
    "    head_list=[]\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver =webdriver.Chrome(executable_path =driver_path, options =options)\n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1020&nso=&is_sug_officeid=0'\n",
    "        url = url_domain + query + '&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    " \n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "    \n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "            \n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "    \n",
    "    \n",
    "        for url in url_list:\n",
    "            driver = webdriver.Chrome(executable_path=driver_path,options=options)\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                time.sleep(1)\n",
    "            except:\n",
    "                print('link error')\n",
    "                continue\n",
    "        # try문 안에서 각자 언론사에 맞게 수정\n",
    "            try:\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(soup.select_one('#container > div.article_title > h1').text)\n",
    "                tags = soup.select_one('#content > div > div.article_txt').text\n",
    "                tags=clean_text(tags)\n",
    "                data_list.append(tags)\n",
    "            except:\n",
    "                try:\n",
    "                    soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                    head_list.append(soup.select_one('#content > div.article_title > h2').text)\n",
    "                    tags = soup.select_one('#ct').text\n",
    "                    tags=clean_text(tags)\n",
    "                    data_list.append(tags)\n",
    "                except:\n",
    "                    print(url)\n",
    "                    print('None_type')\n",
    "            driver.close()\n",
    "        current_page += 10\n",
    "        \n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 매일일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_i(query,driver_path,page):\n",
    "    domain = '매일일보'\n",
    "    driver = webdriver.Chrome(executable_path=driver_path, options=options) \n",
    "    \n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    data_list = []\n",
    "    head_list = []\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path =driver_path, options=options)\n",
    "        url_domain ='https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2385&nso=so%3Ar%2Cp%3Aall%2Ca%3Aall&is_sug_officeid=0'\n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        print(url)\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "    \n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "    \n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "    \n",
    "        except: \n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return ()\n",
    "        driver.close()\n",
    "    \n",
    "        for url in url_list:\n",
    "            driver = webdriver.Chrome(executable_path=driver_path,options=options) \n",
    "            \n",
    "            try:\n",
    "                driver.get(url)\n",
    "                time.sleep(1)\n",
    "            except:\n",
    "                print('link error')\n",
    "                continue\n",
    "        # try문 안에서 각자 언론사에 맞게 수정\n",
    "            try:\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(soup.select('#user-container > div.custom-pc.float-center.max-width-1130 > header > div > div')[0].text)\n",
    "                tags = soup.find(id='article-view-content-div').find_all('p')\n",
    "                data = [clean_text(x.text) for x in tags]\n",
    "                data_list.append(' '.join(data))\n",
    "            except:\n",
    "                print(url)\n",
    "                print('None_type')\n",
    "            driver.close()\n",
    "        current_page += 10\n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문화일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def munhwa(query,driver_path,page):\n",
    "    domain = '문화일보'\n",
    "    \n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    head_list = []\n",
    "    data_list = []\n",
    "    \n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path=driver_path)\n",
    "        driver = webdriver.Chrome(executable_path = driver_path)\n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain = '%EB%89%B4%EC%8A%A4&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1021&nso=&is_sug_officeid=0'\n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "        \n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            \n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "                \n",
    "        except:\n",
    "            print('기사가 없습니다')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "        \n",
    "        \n",
    "        for url in url_list:\n",
    "            driver = webdriver.Chrome(executable_path=driver_path)  # for Windows\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                time.sleep(1)\n",
    "            \n",
    "            except:\n",
    "                print('link error')\n",
    "                continue\n",
    "                # try문 안에서 각자 언론사에 맞게 수정\n",
    "                \n",
    "            try:\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(soup.find('span', class_ = 'title').text)\n",
    "                data = soup.select('#NewsAdContent')[0].text\n",
    "                data=data.strip()\n",
    "                data=data.strip('\\t')\n",
    "                data=clean_text(data)\n",
    "                data_list.append(data)\n",
    "            \n",
    "            except:\n",
    "                try:\n",
    "                    soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                    head_list.append(soup.find('h3', id = 'articleTitle').text)\n",
    "                    data = soup.select('#articleBodyContents')[0].text\n",
    "                    data=data.strip()\n",
    "                    data=data.strip('\\t')\n",
    "                    data=clean_text(data)\n",
    "                    data_list.append(data)\n",
    "                except:\n",
    "                    print(url)\n",
    "                    print(\"None_type\")\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "        \n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 서울신문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seoul(query,driver_path,page):\n",
    "    domain = '서울신문'\n",
    "    driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    data_list = []\n",
    "    head_list = []\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path=driver_path)\n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='%EB%89%B4%EC%8A%A4&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1081&nso=&is_sug_officeid=0' \n",
    "        url = url_domain + query + '&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "        \n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "        \n",
    "        for url in url_list:\n",
    "            driver = webdriver.Chrome(executable_path=driver_path,options=options)  # for Windows\n",
    "            time.sleep(2)\n",
    "            driver.get(url)\n",
    "            time.sleep(2)\n",
    "            # try문 안에서 각자 언론사에 맞게 수정\n",
    "            try:\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(soup.select_one('#viewWrapDiv > div.S20_title > div.S20_article_tit > h1').text)\n",
    "                tags = soup.select_one(\"#atic_txt1\").text\n",
    "                data=tags.strip()\n",
    "                data=data.strip('\\t')\n",
    "                data=clean_text(data)\n",
    "                data_list.append(data)\n",
    "            except:\n",
    "                print(url)\n",
    "                print('None_type')\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "             \n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 세계일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segye(query,driver_path,page):\n",
    "    domain = '세계일보'\n",
    "    driver = webdriver.Chrome(executable_path=driver_path)\n",
    "        \n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    head_list = []\n",
    "    data_list = []\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path=driver_path)\n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1022&nso=&is_sug_officeid=0'\n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page)+ news_domain\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "\n",
    "        for url in url_list:\n",
    "            driver = webdriver.Chrome(executable_path=driver_path) \n",
    "            try:\n",
    "                driver.get(url)\n",
    "                time.sleep(1)\n",
    "            except:\n",
    "                print('link error')\n",
    "                continue\n",
    "                \n",
    "            # try문 안에서 각자 언론사에 맞게 수정\n",
    "            try:\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(soup.select('#title_sns')[0].text)\n",
    "                data = soup.find('article',class_='viewBox').text\n",
    "                data=data.strip()\n",
    "                data=clean_text(data)\n",
    "                data_list.append(data)\n",
    "            except:\n",
    "                print(url)\n",
    "                print('None_type')\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "        \n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 아시아투데이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asiatoday(query,driver_path,page):\n",
    "    domain = '아시아투데이'\n",
    "    driver = webdriver.Chrome(executable_path=driver_path)\n",
    "    \n",
    "    current_page = 1 \n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    head_list = []\n",
    "    data_list = []\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path=driver_path)\n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2268&nso=&is_sug_officeid=0'\n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > sec6tion > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "\n",
    "        for url in url_list:\n",
    "            driver = webdriver.Chrome(executable_path=driver_path)  # for Windows\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                time.sleep(1)\n",
    "            except:\n",
    "                print('link error')\n",
    "                continue\n",
    "            # try문 안에서 각자 언론사에 맞게 수정\n",
    "            try:\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(soup.select('#section_top > div > h3')[0].text)\n",
    "                data = soup.find('div',class_='news_bm').text\n",
    "                data=data.strip()\n",
    "                data=clean_text(data)\n",
    "                data_list.append(data)\n",
    "            except:\n",
    "                print(url)\n",
    "                print('None_type')\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "        \n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전국매일신문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alldays(query,driver_path,page):\n",
    "    head_list = []\n",
    "    data_list = []\n",
    "    current_page = 1 \n",
    "    last_page =(int(page)-1)*10+1\n",
    "    domain = '전국매일신문'\n",
    "    \n",
    "    while current_page < last_page:       \n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options) \n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain = '&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2844&nso=&is_sug_officeid=0'\n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "        for url in url_list:\n",
    "            try:\n",
    "                driver = webdriver.Chrome(executable_path=driver_path, options=options)  # for Windows\n",
    "                driver.get(url)\n",
    "                time.sleep(2)\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head = clean_text(soup.find('div',class_=\"article-head-title\").text)\n",
    "                head_list.append(head)\n",
    "                tags = soup.select_one('#article-view-content-div').find_all('p')\n",
    "                data = [clean_text(x.text) for x in tags]\n",
    "                data_list.append(' '.join(data))\n",
    "            except:\n",
    "                print(url)\n",
    "                print(\"None_type\")\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                           '제목' :  head_list,\n",
    "                           '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 조선일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joseon(query,driver_path,page):\n",
    "    head_list = []\n",
    "    data_list = []\n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    domain = '조선일보'\n",
    "    \n",
    "    while current_page < last_page:        \n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain = '&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1023&nso=&is_sug_officeid=0'\n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "        for url in url_list:\n",
    "            try:\n",
    "                driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "                driver.get(url)\n",
    "                time.sleep(2)\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head = clean_text(soup.find_all(\"h1\", class_=\"article-header__headline | font--primary text--black\")[0].text)\n",
    "                head_list.append(head)\n",
    "                tags = soup.find_all('p',class_=' article-body__content article-body__content-text | text--black text font--size-sm-18 font--size-md-18 font--primary')\n",
    "                data = [clean_text(x.text) for x in tags]\n",
    "                data_list.append(' '.join(data))\n",
    "            except:\n",
    "                print(\"None_type\")\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                           '제목' :  head_list,\n",
    "                           '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 중앙일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chungang(query,driver_path,page):\n",
    "    head_list = []\n",
    "    data_list = []\n",
    "    current_page = 1 \n",
    "    last_page =(int(page)-1)*10+1\n",
    "    domain = '중앙일보'\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options)  \n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain = '&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1025&nso=&is_sug_officeid=0'\n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "        for url in url_list:\n",
    "            try:\n",
    "                driver = webdriver.Chrome(executable_path=driver_path, options=options)  # for Windows\n",
    "                driver.get(url)\n",
    "                time.sleep(2)\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head = clean_text(soup.select('#article_title')[0].text)\n",
    "                head_list.append(head)\n",
    "                text = soup.find('div', id='article_body').text\n",
    "                text = clean_text(text)\n",
    "                data_list.append(text)\n",
    "            except:\n",
    "                print(url)\n",
    "                print(\"None_type\")\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                           '제목' :  head_list,\n",
    "                           '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 천지일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newscj(query,driver_path,page):\n",
    "    head_list = []\n",
    "    data_list = []\n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    domain = '천지일보'\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options) \n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2041&nso=&is_sug_officeid=0' \n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "        for url in url_list:\n",
    "            try:\n",
    "                driver = webdriver.Chrome(executable_path=driver_path, options=options) \n",
    "                driver.get(url)\n",
    "                time.sleep(2)\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head = clean_text(soup.find('div',class_=\"article-head-title\").text)\n",
    "                head_list.append(head)\n",
    "                tags = soup.select_one('#article-view-content-div').find_all('p')\n",
    "                data = [clean_text(x.text) for x in tags]\n",
    "                data_list.append(' '.join(data))\n",
    "            except:\n",
    "                print(url)\n",
    "                print(\"None_type\")\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                           '제목' :  head_list,\n",
    "                           '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한겨례"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hani(query,driver_path,page):\n",
    "    head_list = []\n",
    "    data_list = []\n",
    "    current_page = 1 \n",
    "    last_page =(int(page)-1)*10+1\n",
    "    domain = '한겨례'\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options) \n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1028&nso=&is_sug_officeid=0' \n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "        for url in url_list:\n",
    "            try:\n",
    "                driver = webdriver.Chrome(executable_path=driver_path, options=options)  \n",
    "                driver.get(url)\n",
    "                time.sleep(2)\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head = clean_text(soup.find('span', class_='title').text)\n",
    "                head_list.append(head)\n",
    "                tags = soup.select_one('#a-left-scroll-in > div.article-text > div').find_all('div', class_='text')\n",
    "                data = [clean_text(x.text) for x in tags]\n",
    "                data_list.append(' '.join(data))\n",
    "            except:\n",
    "                print(\"None_type\")\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                           '제목' :  head_list,\n",
    "                           '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한국일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hankook(query,driver_path,page):\n",
    "    domain = '한국일보'\n",
    "    driver = webdriver.Chrome(executable_path=driver_path, options=options)  # for Windows\n",
    "    \n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    head_list = []\n",
    "    data_list = []\n",
    "    \n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path = driver_path)\n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain = '%EB%89%B4%EC%8A%A4&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1469&nso=&is_sug_officeid=0'\n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "        \n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            \n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "                \n",
    "        except:\n",
    "            print('기사가 없')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "        \n",
    "        \n",
    "        for url in url_list:\n",
    "            driver = webdriver.Chrome(executable_path=driver_path)  # for Windows\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                time.sleep(1)\n",
    "            \n",
    "            except:\n",
    "                print('link error')\n",
    "                continue\n",
    "                # try문 안에서 각자 언론사에 맞게 수정\n",
    "                \n",
    "            try:\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(clean_text(soup.find('h2').text))\n",
    "                tags = soup.select_one('body > div.wrap > div.container.end.end-uni > div.end-body > div > div.col-main.read').find_all('p')\n",
    "                data = [x.text for x in tags]\n",
    "                data_list.append(data)\n",
    "            \n",
    "            except:\n",
    "                print(url)\n",
    "                print('None_type')\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "        \n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 원하는 keyword와 페이지 수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '\"풀무원\" 바른먹거리'\n",
    "max_page=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=101&oid=032&aid=0000069224\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=032&aid=0000070370\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=032&aid=0000070370\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=032&aid=0000070370\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=032&aid=0000070370\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=032&aid=0000070370\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=032&aid=0000070370\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=032&aid=0000070370\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=032&aid=0000070370\n",
      "None_type\n"
     ]
    }
   ],
   "source": [
    "biz_df = biz_khan(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://news.kukinews.com/article/view.asp?arcid=0008618427&code=41171811&cp=nv\n",
      "None_type\n",
      "http://news.kukinews.com/article/view.asp?arcid=0008613476&code=41171811&cp=nv\n",
      "None_type\n",
      "http://news.kukinews.com/article/view.asp?arcid=0008568919&code=41171811&cp=nv\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=101&oid=005&aid=0000166450\n",
      "None_type\n",
      "http://news.kukinews.com/article/view.asp?page=1&gCode=cul&arcid=0007343144&cp=nv\n",
      "None_type\n",
      "http://news.kmib.co.kr/article/view.asp?arcid=0008453804&code=41171811&cp=nv\n",
      "None_type\n",
      "http://www.kukinews.com/news2/article/view.asp?page=1&gCode=all&arcid=1237210098&cp=nv\n",
      "None_type\n"
     ]
    }
   ],
   "source": [
    "kmib_df = kmib(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=101&oid=086&aid=0000048442\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=101&oid=086&aid=0002128522\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=086&aid=0002161049\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=101&oid=086&aid=0002168403\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=086&aid=0002152391\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=101&oid=086&aid=0002078723\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=101&oid=086&aid=0001957511\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=101&oid=086&aid=0001938848\n",
      "None_type\n"
     ]
    }
   ],
   "source": [
    "naeil_df = naeil(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "donga_df = donga(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://search.naver.com/search.naver?where=news&query=\"풀무원\" 바른먹거리&%2Ca%3A&start=1&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2385&nso=so%3Ar%2Cp%3Aall%2Ca%3Aall&is_sug_officeid=0\n",
      "https://search.naver.com/search.naver?where=news&query=\"풀무원\" 바른먹거리&%2Ca%3A&start=11&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2385&nso=so%3Ar%2Cp%3Aall%2Ca%3Aall&is_sug_officeid=0\n",
      "https://search.naver.com/search.naver?where=news&query=\"풀무원\" 바른먹거리&%2Ca%3A&start=21&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2385&nso=so%3Ar%2Cp%3Aall%2Ca%3Aall&is_sug_officeid=0\n",
      "https://search.naver.com/search.naver?where=news&query=\"풀무원\" 바른먹거리&%2Ca%3A&start=31&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2385&nso=so%3Ar%2Cp%3Aall%2Ca%3Aall&is_sug_officeid=0\n",
      "https://search.naver.com/search.naver?where=news&query=\"풀무원\" 바른먹거리&%2Ca%3A&start=41&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2385&nso=so%3Ar%2Cp%3Aall%2Ca%3Aall&is_sug_officeid=0\n",
      "https://search.naver.com/search.naver?where=news&query=\"풀무원\" 바른먹거리&%2Ca%3A&start=51&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2385&nso=so%3Ar%2Cp%3Aall%2Ca%3Aall&is_sug_officeid=0\n",
      "https://search.naver.com/search.naver?where=news&query=\"풀무원\" 바른먹거리&%2Ca%3A&start=61&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2385&nso=so%3Ar%2Cp%3Aall%2Ca%3Aall&is_sug_officeid=0\n",
      "https://search.naver.com/search.naver?where=news&query=\"풀무원\" 바른먹거리&%2Ca%3A&start=71&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2385&nso=so%3Ar%2Cp%3Aall%2Ca%3Aall&is_sug_officeid=0\n",
      "https://search.naver.com/search.naver?where=news&query=\"풀무원\" 바른먹거리&%2Ca%3A&start=81&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2385&nso=so%3Ar%2Cp%3Aall%2Ca%3Aall&is_sug_officeid=0\n"
     ]
    }
   ],
   "source": [
    "mi_df = m_i(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "munhwa_df = munhwa(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://biz.seoul.co.kr/news/newsView.php?id=20210315500025&wlog_tag3=naver\n",
      "None_type\n",
      "https://www.seoul.co.kr/news/newsView.php?id=20200612014002&wlog_tag3=naver\n",
      "None_type\n",
      "http://nownews.seoul.co.kr/news/newsView.php?id=20180530601018&wlog_tag3=naver\n",
      "None_type\n",
      "http://nownews.seoul.co.kr/news/newsView.php?id=20180809601005&wlog_tag3=naver\n",
      "None_type\n",
      "http://nownews.seoul.co.kr/news/newsView.php?id=20160414601010\n",
      "None_type\n",
      "http://nownews.seoul.co.kr/news/newsView.php?id=20170612601013&wlog_tag3=naver\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=103&oid=081&aid=0000082014\n",
      "None_type\n",
      "http://nownews.seoul.co.kr/news/newsView.php?id=20100903603077\n",
      "None_type\n"
     ]
    }
   ],
   "source": [
    "seoul_df = seoul(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "link error\n",
      "http://www.economysegye.com/articles/view.html?aid=20130326001515&cid=7111000000000&OutUrl=naver\n",
      "None_type\n",
      "link error\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=022&aid=0000092234\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=022&aid=0000092149\n",
      "None_type\n"
     ]
    }
   ],
   "source": [
    "segye_df = segye(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "asia_df = asiatoday(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기사가 없습니다.\n"
     ]
    }
   ],
   "source": [
    "alldays_df = alldays(query, driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "joseon_df = joseon(query, driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기사가 없습니다.\n"
     ]
    }
   ],
   "source": [
    "chungang_df = chungang(query, driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "cj_df = newscj(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None_type\n",
      "None_type\n"
     ]
    }
   ],
   "source": [
    "hani_df = hani(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "hk_df = hankook(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list=[biz_df, kmib_df, naeil_df, donga_df, mi_df, munhwa_df, seoul_df, segye_df, asia_df, alldays_df ,joseon_df, chungang_df, cj_df, hani_df, hk_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=[]\n",
    "for name in df_list:\n",
    "    if type(name)!=int:\n",
    "        names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df=pd.concat(names)\n",
    "news_df.drop_duplicates(['제목'],inplace=True,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>언론사</th>\n",
       "      <th>제목</th>\n",
       "      <th>내용</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>경향신문</td>\n",
       "      <td>‘바른 먹거리’ 풀무원 물류센터에서는 무슨 일이?</td>\n",
       "      <td>지난 16일 사회관계망서비스에서는 차밑에 들어가 있는 한 노동자의 사진이 화제를 모...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>경향신문</td>\n",
       "      <td>어린이 ‘바른 먹거리 캠페인’ 신청하세요… 풀무원, 사회공헌 교육 확대</td>\n",
       "      <td>풀무원이 어린이들의 바른 식생활을 위해 2010년부터 실시해온 ‘바른 먹거리 캠페인...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>경향신문</td>\n",
       "      <td>풀무원, 어린이 바른 먹거리 현장 체험교육</td>\n",
       "      <td>풀무원 홀딩스는 초등학교 여름방학을 맞아 충북 음성 풀무원 두부공장을 돌아보는 ‘풀...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>경향신문</td>\n",
       "      <td>풀무원 ‘바른 먹을거리 교육’에 어린이 신청 늘어</td>\n",
       "      <td>어린이를 대상으로 한 올바른 먹을거리 교육이 학교와 학부모들로부터 관심을 모으고 있...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>경향신문</td>\n",
       "      <td>풀무원, 13년 만에 새 CI 발표... 글로벌 로하스 도약 목표</td>\n",
       "      <td>풀무원 새 풀무원이 2005년 이후 13년 만에 기업이미지를 교체했다 계열사 브랜드...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>한겨례</td>\n",
       "      <td>“일 25씩 더 해 기부하면 세계 굶주림 사라져”</td>\n",
       "      <td>“세상의 아름다움 중에서 인간의 아름다움을 딛고 넘어설 만한 아름다움은 없다고 생각...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>한국일보</td>\n",
       "      <td>냉면·짜장·떡볶이 제품에 트레이가 필요한가요</td>\n",
       "      <td>[[쓰레기를 사지 않을 권리]&lt;15&gt;즉석조리식품 트레이, 기후위기와 쓰레기산에 신음...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>한국일보</td>\n",
       "      <td>풀무원 입사 1호 사원이 … 성장세 발목 잡던 해외 실적도 개선</td>\n",
       "      <td>[\\n원혜영 의원 부친 농장, 남승우 前대표가 기업화\\n, \\n‘바른 먹거리’로 年...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>한국일보</td>\n",
       "      <td>풀무원 13년 만에 기업이미지 바꿔… 계열사 브랜드도 통일</td>\n",
       "      <td>[풀무원이 2005년 이후 13년 만에 기업이미지(CI)를 교체하고 계열사 브랜드 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>한국일보</td>\n",
       "      <td>위원회 출범하고 도…식품업계 경영 속도</td>\n",
       "      <td>[기업의 비(非)재무적 성과를 뜻하는 ESG(환경·사회·지배구조) 경영이 유통업계의...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>308 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      언론사                                       제목  \\\n",
       "0    경향신문              ‘바른 먹거리’ 풀무원 물류센터에서는 무슨 일이?   \n",
       "1    경향신문  어린이 ‘바른 먹거리 캠페인’ 신청하세요… 풀무원, 사회공헌 교육 확대   \n",
       "2    경향신문                  풀무원, 어린이 바른 먹거리 현장 체험교육   \n",
       "3    경향신문              풀무원 ‘바른 먹을거리 교육’에 어린이 신청 늘어   \n",
       "4    경향신문     풀무원, 13년 만에 새 CI 발표... 글로벌 로하스 도약 목표   \n",
       "..    ...                                      ...   \n",
       "303   한겨례              “일 25씩 더 해 기부하면 세계 굶주림 사라져”   \n",
       "304  한국일보                 냉면·짜장·떡볶이 제품에 트레이가 필요한가요   \n",
       "305  한국일보      풀무원 입사 1호 사원이 … 성장세 발목 잡던 해외 실적도 개선   \n",
       "306  한국일보         풀무원 13년 만에 기업이미지 바꿔… 계열사 브랜드도 통일   \n",
       "307  한국일보                    위원회 출범하고 도…식품업계 경영 속도   \n",
       "\n",
       "                                                    내용  \n",
       "0    지난 16일 사회관계망서비스에서는 차밑에 들어가 있는 한 노동자의 사진이 화제를 모...  \n",
       "1    풀무원이 어린이들의 바른 식생활을 위해 2010년부터 실시해온 ‘바른 먹거리 캠페인...  \n",
       "2    풀무원 홀딩스는 초등학교 여름방학을 맞아 충북 음성 풀무원 두부공장을 돌아보는 ‘풀...  \n",
       "3    어린이를 대상으로 한 올바른 먹을거리 교육이 학교와 학부모들로부터 관심을 모으고 있...  \n",
       "4    풀무원 새 풀무원이 2005년 이후 13년 만에 기업이미지를 교체했다 계열사 브랜드...  \n",
       "..                                                 ...  \n",
       "303  “세상의 아름다움 중에서 인간의 아름다움을 딛고 넘어설 만한 아름다움은 없다고 생각...  \n",
       "304  [[쓰레기를 사지 않을 권리]<15>즉석조리식품 트레이, 기후위기와 쓰레기산에 신음...  \n",
       "305  [\\n원혜영 의원 부친 농장, 남승우 前대표가 기업화\\n, \\n‘바른 먹거리’로 年...  \n",
       "306  [풀무원이 2005년 이후 13년 만에 기업이미지(CI)를 교체하고 계열사 브랜드 ...  \n",
       "307  [기업의 비(非)재무적 성과를 뜻하는 ESG(환경·사회·지배구조) 경영이 유통업계의...  \n",
       "\n",
       "[308 rows x 3 columns]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
