{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-20T05:44:43.573354Z",
     "start_time": "2021-08-20T05:44:43.570436Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "\n",
    "driver_path = 'C:/Users/chaeh/chromedriver.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-20T05:44:43.580863Z",
     "start_time": "2021-08-20T05:44:43.575817Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    cleaned_text = re.sub('[a-zA-Z]', '', text)\n",
    "    cleaned_text = re.sub('[\\{\\}\\[\\]\\/?.,;:|\\)*~`!^\\-_+<>@\\#$%&\\\\\\=\\(\\'\\\"]',\n",
    "                          '', cleaned_text)\n",
    "    cleaned_text = ' '.join(cleaned_text.split())\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-20T05:44:43.630378Z",
     "start_time": "2021-08-20T05:44:43.620833Z"
    }
   },
   "outputs": [],
   "source": [
    "def ppomppu_crawler(query,driver_path,page):\n",
    "    driver = webdriver.Chrome(executable_path=driver_path)\n",
    "    text = []\n",
    "    \n",
    "    for i in range(1,page+1):\n",
    "        post_links = [] \n",
    "        \n",
    "        base_url = 'https://www.ppomppu.co.kr/'\n",
    "        parm = 'search_bbs.php?page_size=50&bbs_cate=2&keyword=' + query + '&order_type=date&search_type=sub_memo&page_no='+str(i)\n",
    "        url = base_url + parm\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "        \n",
    "        for j in range(1,51):\n",
    "            if soup.select('body > div > div.contents > div.container > div > form > div > div:nth-of-type('+str(j)+') > div > p.desc > span:nth-of-type(1)')[0].text != '[뽐뿌게시판]':\n",
    "                try:\n",
    "                    if '예고' not in soup.select('body > div > div.contents > div.container > div > form > div > div:nth-of-type('+str(j)+') > div > span > a')[0].text:\n",
    "                        if '유안타' not in soup.select('body > div > div.contents > div.container > div > form > div > div:nth-of-type('+str(j)+') > div > span > a')[0].text:\n",
    "                            post_links.append(soup.select('body > div > div.contents > div.container > div > form > div > div:nth-of-type('+str(j)+') > div > span > a'))\n",
    "                except:\n",
    "                    print('error')\n",
    "        for k in range(len(post_links)):\n",
    "            try:\n",
    "                base_url = 'https://www.ppomppu.co.kr/'\n",
    "                link_url = base_url + post_links[k][0].get('href')\n",
    "                driver.get(link_url)\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                temp = soup.find_all('td',class_='board-contents')\n",
    "                text.append(clean_text(temp[0].text))\n",
    "            except:\n",
    "                print('error link : ',post_links[k])\n",
    "\n",
    "    driver.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-20T05:44:43.644011Z",
     "start_time": "2021-08-20T05:44:43.635247Z"
    }
   },
   "outputs": [],
   "source": [
    "def fmkorea_document(query,driver_path,page):\n",
    "    driver = webdriver.Chrome(executable_path=driver_path)\n",
    "    data_list=[]\n",
    "\n",
    "    for i in range(1,page+1):\n",
    "        base_url = 'https://www.fmkorea.com/?act=IS&is_keyword='\n",
    "        parm = query+'&mid=home&where=document&page='+str(i)\n",
    "        url = base_url + parm\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "        for j in range(1,11):\n",
    "            regex=\"\\[[^]]*\\]\"\n",
    "            title=soup.select('#content > div > ul.searchResult > li:nth-of-type('+str(j)+') > dl > dt > a')\n",
    "            title=title[0].text\n",
    "            title=re.sub(regex,'',str(title))\n",
    "            title=clean_text(title)\n",
    "            \n",
    "            contents=soup.select('#content > div > ul.searchResult > li:nth-of-type('+str(j)+') > dl > dd')\n",
    "            contents=clean_text(contents[0].text)\n",
    "            data_list.append(title+contents)\n",
    "    driver.close()\n",
    "\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-20T05:44:43.659972Z",
     "start_time": "2021-08-20T05:44:43.651351Z"
    }
   },
   "outputs": [],
   "source": [
    "def instiz(query,driver_path,page):\n",
    "    driver = webdriver.Chrome(executable_path=driver_path)\n",
    "    data_list=[]\n",
    "    \n",
    "    for i in range(1,page+1):\n",
    "        base_url = 'https://www.instiz.net/popup_search.htm#gsc.tab=0&gsc.q='\n",
    "        parm = query+'&gsc.page='+str(i)\n",
    "        url = base_url + parm\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "        title = ''\n",
    "        content = ''\n",
    "        comment = ''\n",
    "        for j in range(1,11):\n",
    "            content_url=soup.select('#___gcse_0 > div > div > div > div.gsc-wrapper > div.gsc-resultsbox-visible > div.gsc-resultsRoot.gsc-tabData.gsc-tabdActive > div > div.gsc-expansionArea > div:nth-of-type('+str(j)+') > div.gs-webResult.gs-result > div.gsc-thumbnail-inside > div > a')[0].get('href')\n",
    "            driver.get(str(content_url))\n",
    "            soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "            try:\n",
    "                title=soup.select('#nowsubject > a')[0].text\n",
    "                content=soup.select('#memo_content_1')[0].text\n",
    "                content=content.replace('  에 게시된 글입니다','')\n",
    "                content=content.replace('← 빈공간을 더블탭 해보세요 →','')\n",
    "                comment=soup.select('#ajax_table > tbody')[0].text\n",
    "                comment=clean_text(comment)\n",
    "                #임의로 전처리...\n",
    "                comment=comment.replace('•••','')\n",
    "                comment=comment.replace('답글 스크랩 신고','')\n",
    "            except:\n",
    "                print(\"\")\n",
    "            driver.get(url)\n",
    "            soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "            data_list.append(title+content+comment)\n",
    "    driver.close()\n",
    "\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-20T05:44:43.673312Z",
     "start_time": "2021-08-20T05:44:43.662614Z"
    }
   },
   "outputs": [],
   "source": [
    "def nate_pann_crawler(query,driver_path,page):\n",
    "    driver = webdriver.Chrome(executable_path=driver_path)\n",
    "    text = []\n",
    "    \n",
    "    for i in range(1,page+1):\n",
    "        post_links = [] \n",
    "        \n",
    "        base_url = 'https://pann.nate.com'\n",
    "        parm = '/search/talk?q=' + query + '&sort=DD&page='+str(i)\n",
    "        url = base_url + parm\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "        \n",
    "        for j in range(1,11):\n",
    "            if '주식뉴스모음' not in soup.select('#container > div.content.sub > div.srcharea > div.srch_list.section > ul > li:nth-of-type('+str(j)+') > div.tit > a')[0].text:\n",
    "                post_links.append(soup.select('#container > div.content.sub > div.srcharea > div.srch_list.section > ul > li:nth-of-type('+str(j)+') > div.tit > a'))\n",
    "    \n",
    "        for k in range(len(post_links)):\n",
    "            try:\n",
    "                link_url = base_url + post_links[k][0].get('href')\n",
    "                driver.get(link_url)\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                temp = soup.select('#contentArea')[0].text\n",
    "                text.append(clean_text(temp))\n",
    "            except:\n",
    "                print('error link : ',post_links[k])\n",
    "\n",
    "    driver.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-20T05:44:43.685389Z",
     "start_time": "2021-08-20T05:44:43.676793Z"
    }
   },
   "outputs": [],
   "source": [
    "def dc(query, driver_path, page):\n",
    "    \n",
    "    driver = webdriver.Chrome(executable_path=driver_path)\n",
    "    \n",
    "    text= []\n",
    "    \n",
    "    for i in range(1, page+1):\n",
    "        post_links = []\n",
    "        \n",
    "        base_url = 'https://search.dcinside.com/post/q/.'\n",
    "        \n",
    "        url = base_url + query + '/p/'+str(i)\n",
    "       \n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        for j in range(1,26):\n",
    "            post_links.append(soup.select('#container > div > section.center_content > div.inner > div.integrate_cont.sch_result.result_all > ul > li:nth-of-type('+str(j)+') > a'))\n",
    "            \n",
    "            \n",
    "        for k in range(len(post_links)):\n",
    "            \n",
    "            try:\n",
    "                link_url =  post_links[k][0].get('href')\n",
    "                driver.get(link_url)\n",
    "                soup = BeautifulSoup(driver.page_source , 'html.parser')\n",
    "                temp = soup.find_all('div',class_='write_div')[0].text\n",
    "                \n",
    "                text.append(clean_text(temp))\n",
    "            except:\n",
    "                \n",
    "                print('error_link: ', post_links[k])\n",
    "    driver.close()\n",
    "    return text\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-20T05:44:43.699253Z",
     "start_time": "2021-08-20T05:44:43.692057Z"
    }
   },
   "outputs": [],
   "source": [
    "def ruli(query, driver_path, page):\n",
    "    driver = webdriver.Chrome(executable_path=driver_path)\n",
    "    text= []\n",
    "    \n",
    "    for i in range(1, page+1):\n",
    "        base_url = 'https://bbs.ruliweb.com/search?q='\n",
    "        url = base_url + query + '&page='+str(i) +'&c_page='+str(i)+'#comment_search&gsc.tab=0&gsc.q='+query+'&gsc.page=1'\n",
    "        driver.get(url)\n",
    "        driver.refresh()\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        for j in range(1,16):\n",
    "            comment=soup.select('#comment_search > div > ul > li:nth-of-type('+str(j)+') > div > div > a.title.text_over')\n",
    "            comment=comment[0].text\n",
    "            comment=clean_text(comment)\n",
    "            text.append(comment)\n",
    "            title = soup.select('#board_search > div > ul > li:nth-of-type('+str(j)+') > div > div > a.title.text_over')\n",
    "            title= title[0].text\n",
    "            tilte= clean_text(title) #게시물에 신문기사가 너무 많아서 우선 게시물의 제목만 가져왔습니다\n",
    "            text.append(title)\n",
    "    driver.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-20T05:55:11.833014Z",
     "start_time": "2021-08-20T05:45:45.955871Z"
    }
   },
   "outputs": [],
   "source": [
    "text = []\n",
    "key_word = '환경오염'\n",
    "page = 5\n",
    "\n",
    "text.extend(ppomppu_crawler(key_word,driver_path,page))\n",
    "text.extend(fmkorea_document(key_word,driver_path,page))\n",
    "text.extend(instiz(key_word,driver_path,page))\n",
    "text.extend(nate_pann_crawler(key_word,driver_path,page))\n",
    "text.extend(dc(key_word,driver_path,page))\n",
    "text.extend(ruli(key_word,driver_path,page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-20T05:55:11.835966Z",
     "start_time": "2021-08-20T05:46:02.257Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-20T05:55:11.837606Z",
     "start_time": "2021-08-20T05:47:20.144Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame(text,columns=['내용'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-20T05:55:11.839106Z",
     "start_time": "2021-08-20T05:47:27.874Z"
    }
   },
   "outputs": [],
   "source": [
    "drop_index = df[df['내용']==\"\"].index\n",
    "df.drop(drop_index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-20T05:44:43.764107Z",
     "start_time": "2021-08-20T05:44:43.600Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('커뮤니티_크롤링_모음.csv',encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
