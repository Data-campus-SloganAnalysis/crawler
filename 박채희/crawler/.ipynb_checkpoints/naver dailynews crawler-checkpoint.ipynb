{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 크롤러 환경설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"headless\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각자 chromedriver.exe 경로 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_path = 'C:/Users/chaeh/chromedriver.exe'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특수문자 및 영어등 텍스트 전처리 함수 -> clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    cleaned_text = re.sub('[a-zA-Z]', '', text)\n",
    "    cleaned_text = re.sub('[\\{\\}\\[\\]\\/?,;:|\\)*~`!^\\-_+<>@\\#$%&\\\\\\=\\(\\'\\\"]',\n",
    "                          '', cleaned_text)\n",
    "    cleaned_text = ' '.join(cleaned_text.split())\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 경향신문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biz_khan(query,driver_path,page):\n",
    "    head_list = []\n",
    "    data_list = []\n",
    "    current_page = 1 \n",
    "    last_page =(int(page)-1)*10+1\n",
    "    domain = '경향신문'\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options) \n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1032&nso=&is_sug_officeid=0' \n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "        \n",
    "        for url in url_list:\n",
    "            if url=='None':\n",
    "                continue\n",
    "            driver = webdriver.Chrome(executable_path=driver_path, options=options)  # for Windows\n",
    "            time.sleep(2)\n",
    "            driver.get(url)\n",
    "            time.sleep(2)\n",
    "            # try문 안에서 각자 언론사에 맞게 수정\n",
    "            try:\n",
    "                soup.find('h1',id='articleTtitle')\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(soup.find('h1',id='articleTtitle').text)\n",
    "                tags = soup.select_one('#container > div.main_container > div.art_cont > div.art_body').find_all('p')\n",
    "                data = [clean_text(x.text) for x in tags]\n",
    "                data_list.append(' '.join(data))\n",
    "            except:\n",
    "                try:\n",
    "                    soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                    head_list.append(soup.find('h1',id='article_title').text)\n",
    "                    tags = soup.select_one('#articleBody').find_all('p')\n",
    "                    data = [clean_text(x.text) for x in tags]\n",
    "                    data_list.append(' '.join(data))\n",
    "                except:\n",
    "                    print(url)\n",
    "                    print('None_type')\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "\n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 국민일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmib(query,driver_path,page):\n",
    "    domain = '국민일보'\n",
    "    driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    data_list = []\n",
    "    head_list = []\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1005&nso=so%3Ar%2Cp%3Aall&is_sug_officeid=0' \n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        #print(url)\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "        \n",
    "        for url in url_list:\n",
    "            if url=='None':\n",
    "                continue\n",
    "            driver = webdriver.Chrome(executable_path=driver_path,options=options)  # for Windows\n",
    "            time.sleep(2)\n",
    "            driver.get(url)\n",
    "            time.sleep(2)\n",
    "            # try문 안에서 각자 언론사에 맞게 수정\n",
    "            try:\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(soup.select_one('#sub > div.sub_header > div > div.nwsti > h3').text)\n",
    "                tags = soup.select_one(\"#articleBody\").text\n",
    "                data=tags.strip()\n",
    "                data=data.strip('\\t')\n",
    "                data=clean_text(data)\n",
    "                data_list.append(data)\n",
    "            except:\n",
    "                print(url)\n",
    "                print('None_type')\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "             \n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 내일신문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naeil(query,driver_path,page):\n",
    "    domain = '내일신문'\n",
    "    driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    data_list = []\n",
    "    head_list = []\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2312&nso=&is_sug_officeid=0' \n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        #print(url)\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "        for url in url_list:\n",
    "            if url=='None':\n",
    "                continue\n",
    "            driver = webdriver.Chrome(executable_path=driver_path,options=options)  # for Windows\n",
    "            time.sleep(2)\n",
    "            driver.get(url)\n",
    "            time.sleep(2)\n",
    "            # try문 안에서 각자 언론사에 맞게 수정\n",
    "            try:\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(soup.select_one('#contentArea > div.caL2 > div > div.articleArea > h3').text)\n",
    "                tags = soup.select_one(\"#contents\").text\n",
    "                data=tags.strip()\n",
    "                data=data.strip('\\t')\n",
    "                data=clean_text(data)\n",
    "                data_list.append(data)\n",
    "            except:\n",
    "                print(url)\n",
    "                print('None_type')\n",
    "            driver.close()\n",
    "        current_page+=10  \n",
    "        \n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 동아일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def donga(query,driver_path, page):\n",
    "    domain = '동아일보'\n",
    "    driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "    \n",
    "    current_page =1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    data_list =[]\n",
    "    head_list=[]\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver =webdriver.Chrome(executable_path =driver_path, options =options)\n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1020&nso=&is_sug_officeid=0'\n",
    "        url = url_domain + query + '&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    " \n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "    \n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "            \n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "    \n",
    "    \n",
    "        for url in url_list:\n",
    "            if url=='None':\n",
    "                continue\n",
    "            driver = webdriver.Chrome(executable_path=driver_path,options=options)\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                time.sleep(1)\n",
    "            except:\n",
    "                print('link error')\n",
    "                continue\n",
    "        # try문 안에서 각자 언론사에 맞게 수정\n",
    "            try:\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(soup.select_one('#container > div.article_title > h1').text)\n",
    "                tags = soup.select_one('#content > div > div.article_txt').text\n",
    "                tags=clean_text(tags)\n",
    "                data_list.append(tags)\n",
    "            except:\n",
    "                try:\n",
    "                    soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                    head_list.append(soup.select_one('#content > div.article_title > h2').text)\n",
    "                    tags = soup.select_one('#ct').text\n",
    "                    tags=clean_text(tags)\n",
    "                    data_list.append(tags)\n",
    "                except:\n",
    "                    print(url)\n",
    "                    print('None_type')\n",
    "            driver.close()\n",
    "        current_page += 10\n",
    "        \n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 매일일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_i(query,driver_path,page):\n",
    "    domain = '매일일보'\n",
    "    driver = webdriver.Chrome(executable_path=driver_path, options=options) \n",
    "    \n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    data_list = []\n",
    "    head_list = []\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path =driver_path, options=options)\n",
    "        url_domain ='https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2385&nso=so%3Ar%2Cp%3Aall%2Ca%3Aall&is_sug_officeid=0'\n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        print(url)\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "    \n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "    \n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "    \n",
    "        except: \n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return ()\n",
    "        driver.close()\n",
    "    \n",
    "        for url in url_list:\n",
    "            if url=='None':\n",
    "                continue\n",
    "            driver = webdriver.Chrome(executable_path=driver_path,options=options) \n",
    "            \n",
    "            try:\n",
    "                driver.get(url)\n",
    "                time.sleep(1)\n",
    "            except:\n",
    "                print('link error')\n",
    "                continue\n",
    "        # try문 안에서 각자 언론사에 맞게 수정\n",
    "            try:\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(soup.select('#user-container > div.custom-pc.float-center.max-width-1130 > header > div > div')[0].text)\n",
    "                tags = soup.find(id='article-view-content-div').find_all('p')\n",
    "                data = [clean_text(x.text) for x in tags]\n",
    "                data_list.append(' '.join(data))\n",
    "            except:\n",
    "                print(url)\n",
    "                print('None_type')\n",
    "            driver.close()\n",
    "        current_page += 10\n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문화일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def munhwa(query,driver_path,page):\n",
    "    domain = '문화일보'\n",
    "    \n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    head_list = []\n",
    "    data_list = []\n",
    "    \n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain = '%EB%89%B4%EC%8A%A4&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1021&nso=&is_sug_officeid=0'\n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "        \n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            \n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "                \n",
    "        except:\n",
    "            print('기사가 없습니다')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "        \n",
    "        \n",
    "        for url in url_list:\n",
    "            if url=='None':\n",
    "                continue\n",
    "            driver = webdriver.Chrome(executable_path=driver_path)  # for Windows\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                time.sleep(1)\n",
    "            \n",
    "            except:\n",
    "                print('link error')\n",
    "                continue\n",
    "                # try문 안에서 각자 언론사에 맞게 수정\n",
    "                \n",
    "            try:\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(soup.find('span', class_ = 'title').text)\n",
    "                data = soup.select('#NewsAdContent')[0].text\n",
    "                data=data.strip()\n",
    "                data=data.strip('\\t')\n",
    "                data=clean_text(data)\n",
    "                data_list.append(data)\n",
    "            \n",
    "            except:\n",
    "                try:\n",
    "                    soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                    head_list.append(soup.find('h3', id = 'articleTitle').text)\n",
    "                    data = soup.select('#articleBodyContents')[0].text\n",
    "                    data=data.strip()\n",
    "                    data=data.strip('\\t')\n",
    "                    data=clean_text(data)\n",
    "                    data_list.append(data)\n",
    "                except:\n",
    "                    print(url)\n",
    "                    print(\"None_type\")\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "        \n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 서울신문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seoul(query,driver_path,page):\n",
    "    domain = '서울신문'\n",
    "    driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    data_list = []\n",
    "    head_list = []\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='%EB%89%B4%EC%8A%A4&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1081&nso=&is_sug_officeid=0' \n",
    "        url = url_domain + query + '&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "        \n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "        \n",
    "        for url in url_list:\n",
    "            if url=='None':\n",
    "                continue\n",
    "            driver = webdriver.Chrome(executable_path=driver_path,options=options)  # for Windows\n",
    "            time.sleep(2)\n",
    "            driver.get(url)\n",
    "            time.sleep(2)\n",
    "            # try문 안에서 각자 언론사에 맞게 수정\n",
    "            try:\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(soup.select_one('#viewWrapDiv > div.S20_title > div.S20_article_tit > h1').text)\n",
    "                tags = soup.select_one(\"#atic_txt1\").text\n",
    "                data=tags.strip()\n",
    "                data=data.strip('\\t')\n",
    "                data=clean_text(data)\n",
    "                data_list.append(data)\n",
    "            except:\n",
    "                print(url)\n",
    "                print('None_type')\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "             \n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 세계일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segye(query,driver_path,page):\n",
    "    domain = '세계일보'\n",
    "    driver = webdriver.Chrome(executable_path=driver_path)\n",
    "        \n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    head_list = []\n",
    "    data_list = []\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1022&nso=&is_sug_officeid=0'\n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page)+ news_domain\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "\n",
    "        for url in url_list:\n",
    "            if url=='None':\n",
    "                continue\n",
    "            driver = webdriver.Chrome(executable_path=driver_path) \n",
    "            try:\n",
    "                driver.get(url)\n",
    "                time.sleep(1)\n",
    "            except:\n",
    "                print('link error')\n",
    "                continue\n",
    "                \n",
    "            # try문 안에서 각자 언론사에 맞게 수정\n",
    "            try:\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(soup.select('#title_sns')[0].text)\n",
    "                data = soup.find('article',class_='viewBox').text\n",
    "                data=data.strip()\n",
    "                data=clean_text(data)\n",
    "                data_list.append(data)\n",
    "            except:\n",
    "                print(url)\n",
    "                print('None_type')\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "        \n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 아시아투데이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asiatoday(query,driver_path,page):\n",
    "    domain = '아시아투데이'\n",
    "    driver = webdriver.Chrome(executable_path=driver_path)\n",
    "    \n",
    "    current_page = 1 \n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    head_list = []\n",
    "    data_list = []\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2268&nso=&is_sug_officeid=0'\n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > sec6tion > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "\n",
    "        for url in url_list:\n",
    "            if url=='None':\n",
    "                continue\n",
    "            driver = webdriver.Chrome(executable_path=driver_path)  # for Windows\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                time.sleep(1)\n",
    "            except:\n",
    "                print('link error')\n",
    "                continue\n",
    "            # try문 안에서 각자 언론사에 맞게 수정\n",
    "            try:\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(soup.select('#section_top > div > h3')[0].text)\n",
    "                data = soup.find('div',class_='news_bm').text\n",
    "                data=data.strip()\n",
    "                data=clean_text(data)\n",
    "                data_list.append(data)\n",
    "            except:\n",
    "                print(url)\n",
    "                print('None_type')\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "        \n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전국매일신문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alldays(query,driver_path,page):\n",
    "    head_list = []\n",
    "    data_list = []\n",
    "    current_page = 1 \n",
    "    last_page =(int(page)-1)*10+1\n",
    "    domain = '전국매일신문'\n",
    "    \n",
    "    while current_page < last_page:       \n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options) \n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain = '&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2844&nso=&is_sug_officeid=0'\n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "        for url in url_list:\n",
    "            if url=='None':\n",
    "                continue\n",
    "            try:\n",
    "                driver = webdriver.Chrome(executable_path=driver_path, options=options)  # for Windows\n",
    "                driver.get(url)\n",
    "                time.sleep(2)\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head = clean_text(soup.find('div',class_=\"article-head-title\").text)\n",
    "                head_list.append(head)\n",
    "                tags = soup.select_one('#article-view-content-div').find_all('p')\n",
    "                data = [clean_text(x.text) for x in tags]\n",
    "                data_list.append(' '.join(data))\n",
    "            except:\n",
    "                print(url)\n",
    "                print(\"None_type\")\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                           '제목' :  head_list,\n",
    "                           '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 조선일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joseon(query,driver_path,page):\n",
    "    head_list = []\n",
    "    data_list = []\n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    domain = '조선일보'\n",
    "    \n",
    "    while current_page < last_page:        \n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain = '&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1023&nso=&is_sug_officeid=0'\n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "        for url in url_list:\n",
    "            if url=='None':\n",
    "                continue\n",
    "            try:\n",
    "                driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "                driver.get(url)\n",
    "                time.sleep(2)\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head = clean_text(soup.find_all(\"h1\", class_=\"article-header__headline | font--primary text--black\")[0].text)\n",
    "                head_list.append(head)\n",
    "                tags = soup.find_all('p',class_=' article-body__content article-body__content-text | text--black text font--size-sm-18 font--size-md-18 font--primary')\n",
    "                data = [clean_text(x.text) for x in tags]\n",
    "                data_list.append(' '.join(data))\n",
    "            except:\n",
    "                print(\"None_type\")\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                           '제목' :  head_list,\n",
    "                           '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 중앙일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chungang(query,driver_path,page):\n",
    "    head_list = []\n",
    "    data_list = []\n",
    "    current_page = 1 \n",
    "    last_page =(int(page)-1)*10+1\n",
    "    domain = '중앙일보'\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options)  \n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain = '&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1025&nso=&is_sug_officeid=0'\n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "        for url in url_list:\n",
    "            if url=='None':\n",
    "                continue\n",
    "            try:\n",
    "                driver = webdriver.Chrome(executable_path=driver_path, options=options)  # for Windows\n",
    "                driver.get(url)\n",
    "                time.sleep(2)\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head = clean_text(soup.select('#article_title')[0].text)\n",
    "                head_list.append(head)\n",
    "                text = soup.find('div', id='article_body').text\n",
    "                text = clean_text(text)\n",
    "                data_list.append(text)\n",
    "            except:\n",
    "                print(url)\n",
    "                print(\"None_type\")\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                           '제목' :  head_list,\n",
    "                           '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 천지일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newscj(query,driver_path,page):\n",
    "    head_list = []\n",
    "    data_list = []\n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    domain = '천지일보'\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options) \n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2041&nso=&is_sug_officeid=0' \n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "        for url in url_list:\n",
    "            if url=='None':\n",
    "                continue\n",
    "            try:\n",
    "                driver = webdriver.Chrome(executable_path=driver_path, options=options) \n",
    "                driver.get(url)\n",
    "                time.sleep(2)\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head = clean_text(soup.find('div',class_=\"article-head-title\").text)\n",
    "                head_list.append(head)\n",
    "                tags = soup.select_one('#article-view-content-div').find_all('p')\n",
    "                data = [clean_text(x.text) for x in tags]\n",
    "                data_list.append(' '.join(data))\n",
    "            except:\n",
    "                print(url)\n",
    "                print(\"None_type\")\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                           '제목' :  head_list,\n",
    "                           '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한겨례"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hani(query,driver_path,page):\n",
    "    head_list = []\n",
    "    data_list = []\n",
    "    current_page = 1 \n",
    "    last_page =(int(page)-1)*10+1\n",
    "    domain = '한겨례'\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options) \n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1028&nso=&is_sug_officeid=0' \n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "        for url in url_list:\n",
    "            if url=='None':\n",
    "                continue\n",
    "            try:\n",
    "                driver = webdriver.Chrome(executable_path=driver_path, options=options)  \n",
    "                driver.get(url)\n",
    "                time.sleep(2)\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head = clean_text(soup.find('span', class_='title').text)\n",
    "                head_list.append(head)\n",
    "                tags = soup.select_one('#a-left-scroll-in > div.article-text > div').find_all('div', class_='text')\n",
    "                data = [clean_text(x.text) for x in tags]\n",
    "                data_list.append(' '.join(data))\n",
    "            except:\n",
    "                print(\"None_type\")\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                           '제목' :  head_list,\n",
    "                           '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한국일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hankook(query,driver_path,page):\n",
    "    domain = '한국일보'\n",
    "    driver = webdriver.Chrome(executable_path=driver_path, options=options)  # for Windows\n",
    "    \n",
    "    current_page = 3\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    head_list = []\n",
    "    data_list = []\n",
    "    \n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path = driver_path)\n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain = '%EB%89%B4%EC%8A%A4&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1469&nso=&is_sug_officeid=0'\n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "        \n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "        for url in url_list:\n",
    "            if url=='None':\n",
    "                continue\n",
    "            driver = webdriver.Chrome(executable_path=driver_path, options=options)  # for Windows\n",
    "            driver.get(url)\n",
    "            time.sleep(2)\n",
    "            # try문 안에서 각자 언론사에 맞게 수정\n",
    "            try:\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(clean_text(soup.find('h2').text))\n",
    "                tags = soup.select_one('body > div.wrap > div.container.end.end-uni > div.end-body > div > div.col-main.read').find_all('p')\n",
    "                data = [clean_text(x.text) for x in tags]\n",
    "                if data=='' or data==None or data==' ':\n",
    "                    data='null'\n",
    "                data_list.append(data)\n",
    "            except:\n",
    "                print(url)\n",
    "                print('None_type')\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "        \n",
    "    print(len(head_list))\n",
    "    print(len(data_list))\n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 원하는 keyword와 페이지 수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '환경오염'\n",
    "max_page=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "biz_df = biz_khan(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmib_df = kmib(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=086&aid=0002165641\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=086&aid=0002164744\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=086&aid=0002163003\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=086&aid=0002160698\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=104&oid=086&aid=0002135772\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=086&aid=0002138344\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=086&aid=0002060529\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=086&aid=0002056690\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=110&oid=086&aid=0002054359\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=086&aid=0002054133\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=086&aid=0002005529\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=086&aid=0001972246\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=086&aid=0001963639\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=086&aid=0002112641\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=086&aid=0001959414\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=086&aid=0001958893\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=086&aid=0001941257\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&oid=086&aid=0000079457\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=086&aid=0000078587\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=086&aid=0000074524\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=086&aid=0000070715\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=086&aid=0000068856\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=086&aid=0000057482\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=086&aid=0000052886\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=086&aid=0000035223\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=086&aid=0002110835\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=104&oid=086&aid=0000056412\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=101&oid=086&aid=0000056261\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=086&aid=0000010654\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=086&aid=0000008709\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=101&oid=086&aid=0000002882\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=101&oid=086&aid=0002035284\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=104&oid=086&aid=0002068419\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=086&aid=0000004558\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=086&aid=0000034700\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=086&aid=0000053430\n",
      "None_type\n"
     ]
    }
   ],
   "source": [
    "naeil_df = naeil(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://news.donga.com/DKBNEWS/3/all/20150102/68879510/3\n",
      "None_type\n",
      "link error\n",
      "link error\n",
      "link error\n"
     ]
    }
   ],
   "source": [
    "donga_df = donga(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://search.naver.com/search.naver?where=news&query=환경오염&%2Ca%3A&start=1&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2385&nso=so%3Ar%2Cp%3Aall%2Ca%3Aall&is_sug_officeid=0\n",
      "https://search.naver.com/search.naver?where=news&query=환경오염&%2Ca%3A&start=11&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2385&nso=so%3Ar%2Cp%3Aall%2Ca%3Aall&is_sug_officeid=0\n",
      "https://search.naver.com/search.naver?where=news&query=환경오염&%2Ca%3A&start=21&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2385&nso=so%3Ar%2Cp%3Aall%2Ca%3Aall&is_sug_officeid=0\n",
      "https://search.naver.com/search.naver?where=news&query=환경오염&%2Ca%3A&start=31&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2385&nso=so%3Ar%2Cp%3Aall%2Ca%3Aall&is_sug_officeid=0\n",
      "https://search.naver.com/search.naver?where=news&query=환경오염&%2Ca%3A&start=41&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2385&nso=so%3Ar%2Cp%3Aall%2Ca%3Aall&is_sug_officeid=0\n",
      "https://search.naver.com/search.naver?where=news&query=환경오염&%2Ca%3A&start=51&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2385&nso=so%3Ar%2Cp%3Aall%2Ca%3Aall&is_sug_officeid=0\n",
      "https://search.naver.com/search.naver?where=news&query=환경오염&%2Ca%3A&start=61&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2385&nso=so%3Ar%2Cp%3Aall%2Ca%3Aall&is_sug_officeid=0\n",
      "https://search.naver.com/search.naver?where=news&query=환경오염&%2Ca%3A&start=71&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2385&nso=so%3Ar%2Cp%3Aall%2Ca%3Aall&is_sug_officeid=0\n",
      "https://search.naver.com/search.naver?where=news&query=환경오염&%2Ca%3A&start=81&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2385&nso=so%3Ar%2Cp%3Aall%2Ca%3Aall&is_sug_officeid=0\n"
     ]
    }
   ],
   "source": [
    "mi_df = m_i(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.munhwa.com/news/view.html?no=20180110MW144913116882\n",
      "None_type\n"
     ]
    }
   ],
   "source": [
    "munhwa_df = munhwa(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.seoul.co.kr/news/newsView.php?id=20210819011011&wlog_tag3=naver\n",
      "None_type\n",
      "https://www.seoul.co.kr/news/newsView.php?id=20210722011019&wlog_tag3=naver\n",
      "None_type\n",
      "https://www.seoul.co.kr/news/newsView.php?id=20210818012025&wlog_tag3=naver\n",
      "None_type\n",
      "https://www.seoul.co.kr/news/newsView.php?id=20210601011019&wlog_tag3=naver\n",
      "None_type\n",
      "https://www.seoul.co.kr/news/newsView.php?id=20210318500030&wlog_tag3=naver\n",
      "None_type\n",
      "https://www.seoul.co.kr/news/newsView.php?id=20210305024018&wlog_tag3=naver\n",
      "None_type\n",
      "https://nownews.seoul.co.kr/news/newsView.php?id=20210221601004&wlog_tag3=naver\n",
      "None_type\n",
      "https://www.seoul.co.kr/news/newsView.php?id=20201123500163&wlog_tag3=naver\n",
      "None_type\n",
      "https://www.seoul.co.kr/news/newsView.php?id=20201113010023&wlog_tag3=naver\n",
      "None_type\n",
      "https://www.seoul.co.kr/news/newsView.php?id=20201106500229&wlog_tag3=naver\n",
      "None_type\n",
      "https://www.seoul.co.kr/news/newsView.php?id=20201117500209&wlog_tag3=naver\n",
      "None_type\n",
      "https://nownews.seoul.co.kr/news/newsView.php?id=20200828601008&wlog_tag3=naver\n",
      "None_type\n",
      "https://nownews.seoul.co.kr/news/newsView.php?id=20201021601014&wlog_tag3=naver\n",
      "None_type\n",
      "https://www.seoul.co.kr/news/newsView.php?id=20210805014003&wlog_tag3=naver\n",
      "None_type\n",
      "https://www.seoul.co.kr/news/newsView.php?id=20191216500135&wlog_tag3=naver\n",
      "None_type\n",
      "https://www.seoul.co.kr/news/newsView.php?id=20200403011016&wlog_tag3=naver\n",
      "None_type\n",
      "https://www.seoul.co.kr/news/newsView.php?id=20210727500055&wlog_tag3=naver\n",
      "None_type\n",
      "https://nownews.seoul.co.kr/news/newsView.php?id=20190831601005&wlog_tag3=naver\n",
      "None_type\n",
      "https://www.seoul.co.kr/news/newsView.php?id=20190528500005&wlog_tag3=naver\n",
      "None_type\n",
      "https://www.seoul.co.kr/news/newsView.php?id=20200617010011&wlog_tag3=naver\n",
      "None_type\n",
      "https://www.seoul.co.kr/news/newsView.php?id=20191203500133&wlog_tag3=naver\n",
      "None_type\n",
      "http://www.seoul.co.kr/news/newsView.php?id=20190409500011&wlog_tag3=naver\n",
      "None_type\n",
      "https://nownews.seoul.co.kr/news/newsView.php?id=20191024601012&wlog_tag3=naver\n",
      "None_type\n",
      "http://www.seoul.co.kr/news/newsView.php?id=20190117500054&wlog_tag3=naver\n",
      "None_type\n",
      "https://www.seoul.co.kr/news/newsView.php?id=20191016019002&wlog_tag3=naver\n",
      "None_type\n",
      "http://www.seoul.co.kr/news/newsView.php?id=20190214018030&wlog_tag3=naver\n",
      "None_type\n",
      "http://www.seoul.co.kr/news/newsView.php?id=20181123500009&wlog_tag3=naver\n",
      "None_type\n",
      "http://www.seoul.co.kr/news/newsView.php?id=20190210500055&wlog_tag3=naver\n",
      "None_type\n",
      "http://www.seoul.co.kr/news/newsView.php?id=20181125500035&wlog_tag3=naver\n",
      "None_type\n",
      "https://nownews.seoul.co.kr/news/newsView.php?id=20210720601006&wlog_tag3=naver\n",
      "None_type\n",
      "http://www.seoul.co.kr/news/newsView.php?id=20180413011004&wlog_tag3=naver\n",
      "None_type\n",
      "http://www.seoul.co.kr/news/newsView.php?id=20180207500062&wlog_tag3=naver\n",
      "None_type\n",
      "http://www.seoul.co.kr/news/newsView.php?id=20171127500113&wlog_tag3=naver\n",
      "None_type\n",
      "http://www.seoul.co.kr/news/newsView.php?id=20170818011011&wlog_tag3=naver\n",
      "None_type\n",
      "http://nownews.seoul.co.kr/news/newsView.php?id=20171009601001&wlog_tag3=naver\n",
      "None_type\n"
     ]
    }
   ],
   "source": [
    "seoul_df = seoul(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "segye_df = segye(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기사가 없습니다.\n"
     ]
    }
   ],
   "source": [
    "asia_df = asiatoday(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldays_df = alldays(query, driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "joseon_df = joseon(query, driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "chungang_df = chungang(query, driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "cj_df = newscj(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None_type\n",
      "None_type\n",
      "None_type\n",
      "None_type\n",
      "None_type\n",
      "None_type\n",
      "None_type\n",
      "None_type\n"
     ]
    }
   ],
   "source": [
    "hani_df = hani(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "hk_df = hankook(query,driver_path,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 네이버 뉴스 모든 일간지 이름 list (dataframe 합칠때 None인 데이터들은 구별해주기 위한 작업)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list=[biz_df, kmib_df, naeil_df, donga_df, mi_df, munhwa_df, seoul_df, segye_df, asia_df, alldays_df ,joseon_df, chungang_df, cj_df, hani_df, hk_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=[]\n",
    "for name in df_list:\n",
    "    if type(name)!=int:\n",
    "        names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df=pd.concat(names)\n",
    "news_df.drop_duplicates(['제목'],inplace=True,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>언론사</th>\n",
       "      <th>제목</th>\n",
       "      <th>내용</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>경향신문</td>\n",
       "      <td>‘오염 주범’ 오명 벗기 나선 화학업계</td>\n",
       "      <td>롯데케미칼·화학…‘폐플라스틱’ 재활용 등 최근 적극적 행보근본적 해결책은 안 돼…‘...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>경향신문</td>\n",
       "      <td>경북도, 소규모 사업장에 환경오염 방지시설비 지원</td>\n",
       "      <td>경북도는 올해부터 8개 시·군 소규모 사업장 100곳에 환경오염 방지시설 유지 및 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>경향신문</td>\n",
       "      <td>대기오염도, 솔잎으로 알 수 있다</td>\n",
       "      <td>환경과학원 잎이 흡수한 오염물질 측정…내년 시범 실시 주변에서 쉽게 볼 수 있는 ‘...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>경향신문</td>\n",
       "      <td>윤성태 휴온스그룹 총수 \"2030년 글로벌기업 100위 안에 이름 올릴 것\"</td>\n",
       "      <td>“휴온스그룹 지주사 휴온스글로벌은 2020년 연결 기준 매출 5230억원 영업이익은...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>경향신문</td>\n",
       "      <td>대구 캠프워커 반환부지 ‘토양오염 정화 용역’···환경자문단 의견도 청취</td>\n",
       "      <td>대구 남구의 캠프워커 반환부지에 대한 토양오염 정화 용역이 다음달부터 실시된다. 국...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101</th>\n",
       "      <td>한국일보</td>\n",
       "      <td>제주도 설 연휴 환경오염 특별감시체계 가동</td>\n",
       "      <td>[제주도청 전경., , , 제주도는 설 연휴기간 환경오염물질 배출사업장에 대한 관리...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1102</th>\n",
       "      <td>한국일보</td>\n",
       "      <td>환경 소녀 툰베리가 보그 표지 모델로 나선 까닭은</td>\n",
       "      <td>[그레타 툰베리가 모델로 등장한 패션 잡지 보그 스칸디나비아판의 표지. 툰베리 트위...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>한국일보</td>\n",
       "      <td>수상태양광 8곳 설치 ... 수질오염 논란 넘을까</td>\n",
       "      <td>[지난 1월 경기 안성 금광호수에 설치된 수상 태양광 발전소 주변 얼음이 절반 정도...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1104</th>\n",
       "      <td>한국일보</td>\n",
       "      <td>오염 차단시설 설치하는데 왜 반대...석포제련소 하천점용 허가 놓고 공방</td>\n",
       "      <td>[경북 봉화군 석포면 영풍석포제련소 모습. 자료사진, 경북 봉화군 영풍 석포제련소가...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>한국일보</td>\n",
       "      <td>영풍 석포제련소 낙동강 오염 원천차단 공사...봉화군 허가는 아직</td>\n",
       "      <td>[경북 봉화군 석포면 영풍석포제련소 전경. 한국일보 자료사진, 경북 봉화군 영풍 석...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1106 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       언론사                                          제목  \\\n",
       "0     경향신문                       ‘오염 주범’ 오명 벗기 나선 화학업계   \n",
       "1     경향신문                 경북도, 소규모 사업장에 환경오염 방지시설비 지원   \n",
       "2     경향신문                          대기오염도, 솔잎으로 알 수 있다   \n",
       "3     경향신문  윤성태 휴온스그룹 총수 \"2030년 글로벌기업 100위 안에 이름 올릴 것\"   \n",
       "4     경향신문    대구 캠프워커 반환부지 ‘토양오염 정화 용역’···환경자문단 의견도 청취   \n",
       "...    ...                                         ...   \n",
       "1101  한국일보                     제주도 설 연휴 환경오염 특별감시체계 가동   \n",
       "1102  한국일보                 환경 소녀 툰베리가 보그 표지 모델로 나선 까닭은   \n",
       "1103  한국일보                 수상태양광 8곳 설치 ... 수질오염 논란 넘을까   \n",
       "1104  한국일보    오염 차단시설 설치하는데 왜 반대...석포제련소 하천점용 허가 놓고 공방   \n",
       "1105  한국일보        영풍 석포제련소 낙동강 오염 원천차단 공사...봉화군 허가는 아직   \n",
       "\n",
       "                                                     내용  \n",
       "0     롯데케미칼·화학…‘폐플라스틱’ 재활용 등 최근 적극적 행보근본적 해결책은 안 돼…‘...  \n",
       "1     경북도는 올해부터 8개 시·군 소규모 사업장 100곳에 환경오염 방지시설 유지 및 ...  \n",
       "2     환경과학원 잎이 흡수한 오염물질 측정…내년 시범 실시 주변에서 쉽게 볼 수 있는 ‘...  \n",
       "3     “휴온스그룹 지주사 휴온스글로벌은 2020년 연결 기준 매출 5230억원 영업이익은...  \n",
       "4     대구 남구의 캠프워커 반환부지에 대한 토양오염 정화 용역이 다음달부터 실시된다. 국...  \n",
       "...                                                 ...  \n",
       "1101  [제주도청 전경., , , 제주도는 설 연휴기간 환경오염물질 배출사업장에 대한 관리...  \n",
       "1102  [그레타 툰베리가 모델로 등장한 패션 잡지 보그 스칸디나비아판의 표지. 툰베리 트위...  \n",
       "1103  [지난 1월 경기 안성 금광호수에 설치된 수상 태양광 발전소 주변 얼음이 절반 정도...  \n",
       "1104  [경북 봉화군 석포면 영풍석포제련소 모습. 자료사진, 경북 봉화군 영풍 석포제련소가...  \n",
       "1105  [경북 봉화군 석포면 영풍석포제련소 전경. 한국일보 자료사진, 경북 봉화군 영풍 석...  \n",
       "\n",
       "[1106 rows x 3 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.to_csv('환경오염.csv', encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('환경오염.csv',encoding='UTF-8',index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=test.drop(['제목'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('환경오염.csv', encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('환경오염.csv',encoding='UTF-8',index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    cleaned_text = re.sub('[^ ㄱ-ㅣ가-힣]+',' ',text) # 한국어, 스페이스바를 제외한 모든 문자 제거\n",
    "    cleaned_text = ' '.join(cleaned_text.split()) # 너무 많은 공백이 생기므로 공백 1개로\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['내용']=test['내용'].apply(lambda x:clean_text(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('환경오염.csv', encoding='CP949')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
