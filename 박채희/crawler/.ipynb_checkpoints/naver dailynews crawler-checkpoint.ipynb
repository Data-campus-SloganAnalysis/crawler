{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 크롤러 환경설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"headless\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각자 chromedriver.exe 경로 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_path = 'C:/Users/chaeh/chromedriver.exe'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특수문자 및 영어등 텍스트 전처리 함수 -> clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    cleaned_text = re.sub('[a-zA-Z]', '', text)\n",
    "    cleaned_text = re.sub('[\\{\\}\\[\\]\\/?,;:|\\)*~`!^\\-_+<>@\\#$%&\\\\\\=\\(\\'\\\"]',\n",
    "                          '', cleaned_text)\n",
    "    cleaned_text = ' '.join(cleaned_text.split())\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 경향신문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biz_khan(query,driver_path,page):\n",
    "    head_list = []\n",
    "    data_list = []\n",
    "    current_page = 1 \n",
    "    last_page =(int(page)-1)*10+1\n",
    "    domain = '경향신문'\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options) \n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1032&nso=&is_sug_officeid=0' \n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "        \n",
    "        for url in url_list:\n",
    "            if url=='None':\n",
    "                continue\n",
    "            driver = webdriver.Chrome(executable_path=driver_path, options=options)  # for Windows\n",
    "            time.sleep(2)\n",
    "            driver.get(url)\n",
    "            time.sleep(2)\n",
    "            # try문 안에서 각자 언론사에 맞게 수정\n",
    "            try:\n",
    "                soup.find('h1',id='articleTtitle')\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(soup.find('h1',id='articleTtitle').text)\n",
    "                tags = soup.select_one('#container > div.main_container > div.art_cont > div.art_body').find_all('p')\n",
    "                data = [clean_text(x.text) for x in tags]\n",
    "                data_list.append(' '.join(data))\n",
    "            except:\n",
    "                try:\n",
    "                    soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                    head_list.append(soup.find('h1',id='article_title').text)\n",
    "                    tags = soup.select_one('#articleBody').find_all('p')\n",
    "                    data = [clean_text(x.text) for x in tags]\n",
    "                    data_list.append(' '.join(data))\n",
    "                except:\n",
    "                    print(url)\n",
    "                    print('None_type')\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "\n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 국민일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmib(query,driver_path,page):\n",
    "    domain = '국민일보'\n",
    "    driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    data_list = []\n",
    "    head_list = []\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1005&nso=so%3Ar%2Cp%3Aall&is_sug_officeid=0' \n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        #print(url)\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "        \n",
    "        for url in url_list:\n",
    "            if url=='None':\n",
    "                continue\n",
    "            driver = webdriver.Chrome(executable_path=driver_path,options=options)  # for Windows\n",
    "            time.sleep(2)\n",
    "            driver.get(url)\n",
    "            time.sleep(2)\n",
    "            # try문 안에서 각자 언론사에 맞게 수정\n",
    "            try:\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(soup.select_one('#sub > div.sub_header > div > div.nwsti > h3').text)\n",
    "                tags = soup.select_one(\"#articleBody\").text\n",
    "                data=tags.strip()\n",
    "                data=data.strip('\\t')\n",
    "                data=clean_text(data)\n",
    "                data_list.append(data)\n",
    "            except:\n",
    "                print(url)\n",
    "                print('None_type')\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "             \n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 내일신문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naeil(query,driver_path,page):\n",
    "    domain = '내일신문'\n",
    "    driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    data_list = []\n",
    "    head_list = []\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2312&nso=&is_sug_officeid=0' \n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        #print(url)\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "        for url in url_list:\n",
    "            if url=='None':\n",
    "                continue\n",
    "            driver = webdriver.Chrome(executable_path=driver_path,options=options)  # for Windows\n",
    "            time.sleep(2)\n",
    "            driver.get(url)\n",
    "            time.sleep(2)\n",
    "            # try문 안에서 각자 언론사에 맞게 수정\n",
    "            try:\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(soup.select_one('#contentArea > div.caL2 > div > div.articleArea > h3').text)\n",
    "                tags = soup.select_one(\"#contents\").text\n",
    "                data=tags.strip()\n",
    "                data=data.strip('\\t')\n",
    "                data=clean_text(data)\n",
    "                data_list.append(data)\n",
    "            except:\n",
    "                print(url)\n",
    "                print('None_type')\n",
    "            driver.close()\n",
    "        current_page+=10  \n",
    "        \n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 동아일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def donga(query,driver_path, page):\n",
    "    domain = '동아일보'\n",
    "    driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "    \n",
    "    current_page =1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    data_list =[]\n",
    "    head_list=[]\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver =webdriver.Chrome(executable_path =driver_path, options =options)\n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1020&nso=&is_sug_officeid=0'\n",
    "        url = url_domain + query + '&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    " \n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "    \n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "            \n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "    \n",
    "    \n",
    "        for url in url_list:\n",
    "            if url=='None':\n",
    "                continue\n",
    "            driver = webdriver.Chrome(executable_path=driver_path,options=options)\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                time.sleep(1)\n",
    "            except:\n",
    "                print('link error')\n",
    "                continue\n",
    "        # try문 안에서 각자 언론사에 맞게 수정\n",
    "            try:\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(soup.select_one('#container > div.article_title > h1').text)\n",
    "                tags = soup.select_one('#content > div > div.article_txt').text\n",
    "                tags=clean_text(tags)\n",
    "                data_list.append(tags)\n",
    "            except:\n",
    "                try:\n",
    "                    soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                    head_list.append(soup.select_one('#content > div.article_title > h2').text)\n",
    "                    tags = soup.select_one('#ct').text\n",
    "                    tags=clean_text(tags)\n",
    "                    data_list.append(tags)\n",
    "                except:\n",
    "                    print(url)\n",
    "                    print('None_type')\n",
    "            driver.close()\n",
    "        current_page += 10\n",
    "        \n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 매일일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_i(query,driver_path,page):\n",
    "    domain = '매일일보'\n",
    "    driver = webdriver.Chrome(executable_path=driver_path, options=options) \n",
    "    \n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    data_list = []\n",
    "    head_list = []\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path =driver_path, options=options)\n",
    "        url_domain ='https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2385&nso=so%3Ar%2Cp%3Aall%2Ca%3Aall&is_sug_officeid=0'\n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        print(url)\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "    \n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "    \n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "    \n",
    "        except: \n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return ()\n",
    "        driver.close()\n",
    "    \n",
    "        for url in url_list:\n",
    "            if url=='None':\n",
    "                continue\n",
    "            driver = webdriver.Chrome(executable_path=driver_path,options=options) \n",
    "            \n",
    "            try:\n",
    "                driver.get(url)\n",
    "                time.sleep(1)\n",
    "            except:\n",
    "                print('link error')\n",
    "                continue\n",
    "        # try문 안에서 각자 언론사에 맞게 수정\n",
    "            try:\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(soup.select('#user-container > div.custom-pc.float-center.max-width-1130 > header > div > div')[0].text)\n",
    "                tags = soup.find(id='article-view-content-div').find_all('p')\n",
    "                data = [clean_text(x.text) for x in tags]\n",
    "                data_list.append(' '.join(data))\n",
    "            except:\n",
    "                print(url)\n",
    "                print('None_type')\n",
    "            driver.close()\n",
    "        current_page += 10\n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문화일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def munhwa(query,driver_path,page):\n",
    "    domain = '문화일보'\n",
    "    \n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    head_list = []\n",
    "    data_list = []\n",
    "    \n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain = '%EB%89%B4%EC%8A%A4&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1021&nso=&is_sug_officeid=0'\n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "        \n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            \n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "                \n",
    "        except:\n",
    "            print('기사가 없습니다')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "        \n",
    "        \n",
    "        for url in url_list:\n",
    "            if url=='None':\n",
    "                continue\n",
    "            driver = webdriver.Chrome(executable_path=driver_path,options=options)  # for Windows\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                time.sleep(1)\n",
    "            \n",
    "            except:\n",
    "                print('link error')\n",
    "                continue\n",
    "                # try문 안에서 각자 언론사에 맞게 수정\n",
    "                \n",
    "            try:\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(soup.find('span', class_ = 'title').text)\n",
    "                data = soup.select('#NewsAdContent')[0].text\n",
    "                data=data.strip()\n",
    "                data=data.strip('\\t')\n",
    "                data=clean_text(data)\n",
    "                data_list.append(data)\n",
    "            \n",
    "            except:\n",
    "                try:\n",
    "                    soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                    head_list.append(soup.find('h3', id = 'articleTitle').text)\n",
    "                    data = soup.select('#articleBodyContents')[0].text\n",
    "                    data=data.strip()\n",
    "                    data=data.strip('\\t')\n",
    "                    data=clean_text(data)\n",
    "                    data_list.append(data)\n",
    "                except:\n",
    "                    print(url)\n",
    "                    print(\"None_type\")\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "        \n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 서울신문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seoul(query,driver_path,page):\n",
    "    domain = '서울신문'\n",
    "    driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    data_list = []\n",
    "    head_list = []\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='%EB%89%B4%EC%8A%A4&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1081&nso=&is_sug_officeid=0' \n",
    "        url = url_domain + query + '&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "        \n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "        \n",
    "        for url in url_list:\n",
    "            if url=='None':\n",
    "                continue\n",
    "            driver = webdriver.Chrome(executable_path=driver_path,options=options)  # for Windows\n",
    "            time.sleep(2)\n",
    "            driver.get(url)\n",
    "            time.sleep(2)\n",
    "            # try문 안에서 각자 언론사에 맞게 수정\n",
    "            try:\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(soup.select_one('#viewWrapDiv > div.S20_title > div.S20_article_tit > h1').text)\n",
    "                tags = soup.select_one(\"#atic_txt1\").text\n",
    "                data=tags.strip()\n",
    "                data=data.strip('\\t')\n",
    "                data=clean_text(data)\n",
    "                data_list.append(data)\n",
    "            except:\n",
    "                print(url)\n",
    "                print('None_type')\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "             \n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 세계일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segye(query,driver_path,page):\n",
    "    domain = '세계일보'\n",
    "    driver = webdriver.Chrome(executable_path=driver_path)\n",
    "        \n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    head_list = []\n",
    "    data_list = []\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1022&nso=&is_sug_officeid=0'\n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page)+ news_domain\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "\n",
    "        for url in url_list:\n",
    "            if url=='None':\n",
    "                continue\n",
    "            driver = webdriver.Chrome(executable_path=driver_path) \n",
    "            try:\n",
    "                driver.get(url)\n",
    "                time.sleep(1)\n",
    "            except:\n",
    "                print('link error')\n",
    "                continue\n",
    "                \n",
    "            # try문 안에서 각자 언론사에 맞게 수정\n",
    "            try:\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(soup.select('#title_sns')[0].text)\n",
    "                data = soup.find('article',class_='viewBox').text\n",
    "                data=data.strip()\n",
    "                data=clean_text(data)\n",
    "                data_list.append(data)\n",
    "            except:\n",
    "                print(url)\n",
    "                print('None_type')\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "        \n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 아시아투데이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asiatoday(query,driver_path,page):\n",
    "    domain = '아시아투데이'\n",
    "    driver = webdriver.Chrome(executable_path=driver_path)\n",
    "    \n",
    "    current_page = 1 \n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    head_list = []\n",
    "    data_list = []\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2268&nso=&is_sug_officeid=0'\n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > sec6tion > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "\n",
    "        for url in url_list:\n",
    "            if url=='None':\n",
    "                continue\n",
    "            driver = webdriver.Chrome(executable_path=driver_path)  # for Windows\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                time.sleep(1)\n",
    "            except:\n",
    "                print('link error')\n",
    "                continue\n",
    "            # try문 안에서 각자 언론사에 맞게 수정\n",
    "            try:\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(soup.select('#section_top > div > h3')[0].text)\n",
    "                data = soup.find('div',class_='news_bm').text\n",
    "                data=data.strip()\n",
    "                data=clean_text(data)\n",
    "                data_list.append(data)\n",
    "            except:\n",
    "                print(url)\n",
    "                print('None_type')\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "        \n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전국매일신문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alldays(query,driver_path,page):\n",
    "    head_list = []\n",
    "    data_list = []\n",
    "    current_page = 1 \n",
    "    last_page =(int(page)-1)*10+1\n",
    "    domain = '전국매일신문'\n",
    "    \n",
    "    while current_page < last_page:       \n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options) \n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain = '&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2844&nso=&is_sug_officeid=0'\n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "        for url in url_list:\n",
    "            if url=='None':\n",
    "                continue\n",
    "            try:\n",
    "                driver = webdriver.Chrome(executable_path=driver_path, options=options)  # for Windows\n",
    "                driver.get(url)\n",
    "                time.sleep(2)\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head = clean_text(soup.find('div',class_=\"article-head-title\").text)\n",
    "                head_list.append(head)\n",
    "                tags = soup.select_one('#article-view-content-div').find_all('p')\n",
    "                data = [clean_text(x.text) for x in tags]\n",
    "                data_list.append(' '.join(data))\n",
    "            except:\n",
    "                print(url)\n",
    "                print(\"None_type\")\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                           '제목' :  head_list,\n",
    "                           '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 조선일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joseon(query,driver_path,page):\n",
    "    head_list = []\n",
    "    data_list = []\n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    domain = '조선일보'\n",
    "    \n",
    "    while current_page < last_page:        \n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain = '&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1023&nso=&is_sug_officeid=0'\n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "        for url in url_list:\n",
    "            if url=='None':\n",
    "                continue\n",
    "            try:\n",
    "                driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "                driver.get(url)\n",
    "                time.sleep(2)\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head = clean_text(soup.find_all(\"h1\", class_=\"article-header__headline | font--primary text--black\")[0].text)\n",
    "                head_list.append(head)\n",
    "                tags = soup.find_all('p',class_=' article-body__content article-body__content-text | text--black text font--size-sm-18 font--size-md-18 font--primary')\n",
    "                data = [clean_text(x.text) for x in tags]\n",
    "                data_list.append(' '.join(data))\n",
    "            except:\n",
    "                print(\"None_type\")\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                           '제목' :  head_list,\n",
    "                           '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 중앙일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chungang(query,driver_path,page):\n",
    "    head_list = []\n",
    "    data_list = []\n",
    "    current_page = 1 \n",
    "    last_page =(int(page)-1)*10+1\n",
    "    domain = '중앙일보'\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options)  \n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain = '&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1025&nso=&is_sug_officeid=0'\n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "        for url in url_list:\n",
    "            if url=='None':\n",
    "                continue\n",
    "            try:\n",
    "                driver = webdriver.Chrome(executable_path=driver_path, options=options)  # for Windows\n",
    "                driver.get(url)\n",
    "                time.sleep(2)\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head = clean_text(soup.select('#article_title')[0].text)\n",
    "                head_list.append(head)\n",
    "                text = soup.find('div', id='article_body').text\n",
    "                text = clean_text(text)\n",
    "                data_list.append(text)\n",
    "            except:\n",
    "                print(url)\n",
    "                print(\"None_type\")\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                           '제목' :  head_list,\n",
    "                           '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 천지일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newscj(query,driver_path,page):\n",
    "    head_list = []\n",
    "    data_list = []\n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    domain = '천지일보'\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options) \n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2041&nso=&is_sug_officeid=0' \n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "        for url in url_list:\n",
    "            if url=='None':\n",
    "                continue\n",
    "            try:\n",
    "                driver = webdriver.Chrome(executable_path=driver_path, options=options) \n",
    "                driver.get(url)\n",
    "                time.sleep(2)\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head = clean_text(soup.find('div',class_=\"article-head-title\").text)\n",
    "                head_list.append(head)\n",
    "                tags = soup.select_one('#article-view-content-div').find_all('p')\n",
    "                data = [clean_text(x.text) for x in tags]\n",
    "                data_list.append(' '.join(data))\n",
    "            except:\n",
    "                print(url)\n",
    "                print(\"None_type\")\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                           '제목' :  head_list,\n",
    "                           '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한겨례"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hani(query,driver_path,page):\n",
    "    head_list = []\n",
    "    data_list = []\n",
    "    current_page = 1 \n",
    "    last_page =(int(page)-1)*10+1\n",
    "    domain = '한겨례'\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options) \n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1028&nso=&is_sug_officeid=0' \n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "        for url in url_list:\n",
    "            if url=='None':\n",
    "                continue\n",
    "            try:\n",
    "                driver = webdriver.Chrome(executable_path=driver_path, options=options)  \n",
    "                driver.get(url)\n",
    "                time.sleep(2)\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head = clean_text(soup.find('span', class_='title').text)\n",
    "                head_list.append(head)\n",
    "                tags = soup.select_one('#a-left-scroll-in > div.article-text > div').find_all('div', class_='text')\n",
    "                data = [clean_text(x.text) for x in tags]\n",
    "                data_list.append(' '.join(data))\n",
    "            except:\n",
    "                print(\"None_type\")\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                           '제목' :  head_list,\n",
    "                           '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한국일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hankook(query,driver_path,page):\n",
    "    domain = '한국일보'\n",
    "    driver = webdriver.Chrome(executable_path=driver_path, options=options)  # for Windows\n",
    "    \n",
    "    current_page = 3\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    head_list = []\n",
    "    data_list = []\n",
    "    \n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path = driver_path,options=options)\n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain = '%EB%89%B4%EC%8A%A4&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1469&nso=&is_sug_officeid=0'\n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "        \n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "        for url in url_list:\n",
    "            if url=='None':\n",
    "                continue\n",
    "            driver = webdriver.Chrome(executable_path=driver_path, options=options)  # for Windows\n",
    "            driver.get(url)\n",
    "            time.sleep(2)\n",
    "            # try문 안에서 각자 언론사에 맞게 수정\n",
    "            try:\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(clean_text(soup.find('h2').text))\n",
    "                tags = soup.select_one('body > div.wrap > div.container.end.end-uni > div.end-body > div > div.col-main.read').find_all('p')\n",
    "                data = [clean_text(x.text) for x in tags]\n",
    "                if data=='' or data==None or data==' ':\n",
    "                    data='null'\n",
    "                data_list.append(data)\n",
    "            except:\n",
    "                print(url)\n",
    "                print('None_type')\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "        \n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 원하는 keyword와 페이지 수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '친환경'\n",
    "max_page=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "biz_df = biz_khan(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmib_df = kmib(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "naeil_df = naeil(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "donga_df = donga(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://search.naver.com/search.naver?where=news&query=친환경&%2Ca%3A&start=1&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2385&nso=so%3Ar%2Cp%3Aall%2Ca%3Aall&is_sug_officeid=0\n",
      "https://search.naver.com/search.naver?where=news&query=친환경&%2Ca%3A&start=11&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2385&nso=so%3Ar%2Cp%3Aall%2Ca%3Aall&is_sug_officeid=0\n",
      "https://search.naver.com/search.naver?where=news&query=친환경&%2Ca%3A&start=21&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2385&nso=so%3Ar%2Cp%3Aall%2Ca%3Aall&is_sug_officeid=0\n",
      "https://search.naver.com/search.naver?where=news&query=친환경&%2Ca%3A&start=31&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2385&nso=so%3Ar%2Cp%3Aall%2Ca%3Aall&is_sug_officeid=0\n"
     ]
    }
   ],
   "source": [
    "mi_df = m_i(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "munhwa_df = munhwa(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.seoul.co.kr/news/newsView.php?id=20210811014015&wlog_tag3=naver\n",
      "None_type\n",
      "https://nownews.seoul.co.kr/news/newsView.php?id=20210808601001&wlog_tag3=naver\n",
      "None_type\n",
      "https://www.seoul.co.kr/news/newsView.php?id=20210806011012&wlog_tag3=naver\n",
      "None_type\n",
      "https://biz.seoul.co.kr/news/newsView.php?id=20210729500119&wlog_tag3=naver\n",
      "None_type\n",
      "https://www.seoul.co.kr/news/newsView.php?id=20210723013001&wlog_tag3=naver\n",
      "None_type\n",
      "https://www.seoul.co.kr/news/newsView.php?id=20210714500172&wlog_tag3=naver\n",
      "None_type\n",
      "https://www.seoul.co.kr/news/newsView.php?id=20210709016001&wlog_tag3=naver\n",
      "None_type\n"
     ]
    }
   ],
   "source": [
    "seoul_df = seoul(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "segye_df = segye(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기사가 없습니다.\n"
     ]
    }
   ],
   "source": [
    "asia_df = asiatoday(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldays_df = alldays(query, driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "joseon_df = joseon(query, driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://news.joins.com/article/olink/23725949\n",
      "None_type\n",
      "https://news.joins.com/article/olink/23724886\n",
      "None_type\n",
      "https://news.joins.com/article/olink/23725619\n",
      "None_type\n",
      "https://news.joins.com/article/olink/23723464\n",
      "None_type\n",
      "https://news.joins.com/article/olink/23721749\n",
      "None_type\n",
      "https://news.joins.com/article/olink/23721753\n",
      "None_type\n",
      "https://news.joins.com/article/olink/23717222\n",
      "None_type\n",
      "https://news.joins.com/article/olink/23712930\n",
      "None_type\n",
      "https://news.joins.com/article/olink/23707806\n",
      "None_type\n",
      "https://news.joins.com/article/olink/23709156\n",
      "None_type\n",
      "https://news.joins.com/article/olink/23726820\n",
      "None_type\n",
      "https://news.joins.com/article/olink/23725942\n",
      "None_type\n",
      "https://news.joins.com/article/olink/23726622\n",
      "None_type\n",
      "https://news.joins.com/article/olink/23726087\n",
      "None_type\n",
      "https://news.joins.com/article/olink/23722741\n",
      "None_type\n",
      "https://news.joins.com/article/olink/23725201\n",
      "None_type\n",
      "https://news.joins.com/article/olink/23723875\n",
      "None_type\n",
      "https://news.joins.com/article/olink/23726563\n",
      "None_type\n",
      "https://news.joins.com/article/olink/23723470\n",
      "None_type\n",
      "https://news.joins.com/article/olink/23722781\n",
      "None_type\n",
      "https://news.joins.com/article/olink/23704290\n",
      "None_type\n",
      "https://news.joins.com/article/olink/23703505\n",
      "None_type\n",
      "https://news.joins.com/article/olink/23723876\n",
      "None_type\n",
      "https://news.joins.com/article/olink/23726175\n",
      "None_type\n",
      "https://news.joins.com/article/olink/23701890\n",
      "None_type\n",
      "https://news.joins.com/article/olink/23697491\n",
      "None_type\n",
      "https://news.joins.com/article/olink/23723335\n",
      "None_type\n",
      "https://news.joins.com/article/olink/23695606\n",
      "None_type\n",
      "https://news.joins.com/article/olink/23695638\n",
      "None_type\n",
      "https://news.joins.com/article/olink/23694374\n",
      "None_type\n",
      "https://news.joins.com/article/olink/23694002\n",
      "None_type\n",
      "https://news.joins.com/article/olink/23700373\n",
      "None_type\n",
      "https://news.joins.com/article/olink/23724608\n",
      "None_type\n",
      "https://news.joins.com/article/olink/23688887\n",
      "None_type\n",
      "https://news.joins.com/article/olink/23688482\n",
      "None_type\n",
      "https://news.joins.com/article/olink/23688985\n",
      "None_type\n",
      "https://news.joins.com/article/olink/23684359\n",
      "None_type\n",
      "https://news.joins.com/article/olink/23684312\n",
      "None_type\n",
      "https://news.joins.com/article/olink/23688897\n",
      "None_type\n",
      "https://news.joins.com/article/olink/23686627\n",
      "None_type\n"
     ]
    }
   ],
   "source": [
    "chungang_df = chungang(query, driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cj_df = newscj(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None_type\n",
      "None_type\n",
      "None_type\n",
      "None_type\n",
      "None_type\n",
      "None_type\n"
     ]
    }
   ],
   "source": [
    "hani_df = hani(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "hk_df = hankook(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 네이버 뉴스 모든 일간지 이름 list (dataframe 합칠때 None인 데이터들은 구별해주기 위한 작업)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list=[biz_df, kmib_df, naeil_df, donga_df, mi_df, munhwa_df, seoul_df, segye_df, asia_df, alldays_df ,joseon_df, chungang_df, cj_df, hani_df, hk_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=[]\n",
    "for name in df_list:\n",
    "    if type(name)!=int:\n",
    "        names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df=pd.concat(names)\n",
    "news_df.drop_duplicates(['제목'],inplace=True,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>언론사</th>\n",
       "      <th>제목</th>\n",
       "      <th>내용</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>경향신문</td>\n",
       "      <td>LG화학, 2조6000억 투자해 친환경 소재 육성한다</td>\n",
       "      <td>화학 대산공장 전경. 화학 제공. 화학이 친환경 소재 사업을 육성하기 위해 대규모 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>경향신문</td>\n",
       "      <td>친환경차 패권 의지 드러낸 미국···한국차 미·중·EU ‘신 경제블록’ 진입 모색해야</td>\n",
       "      <td>조 바이든 미국 대통령이 지난 5일현지시간 미국 워싱턴 백악관에서 열린 무공해차 홍...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>경향신문</td>\n",
       "      <td>중소형 선박에 LPG엔진을···“친환경 시장 주도” 부산 전국 첫 실증 착수</td>\n",
       "      <td>부산시가 중·소형선박 엔진을 액화석유가스 엔진으로 전환하기 위한 실증작업에 나선다....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>경향신문</td>\n",
       "      <td>비정유·친환경 미래 먹거리 찾아나서는 정유업계, ‘탈석유’ 잰걸음</td>\n",
       "      <td>현대오일뱅크는 2030년까지 친환경미래사업의 영업이익 비중을 70까지 확대하기로 했...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>경향신문</td>\n",
       "      <td>화학업계의 친환경 행보? '플라스틱 생산량부터 줄여야'</td>\n",
       "      <td>롯데케미칼 소재 제품 포장백. 롯데케미칼 제공. 과거에는 환경오염의 주범으로 꼽혔던...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>한국일보</td>\n",
       "      <td>실천 기업 우대 친환경기업우대론 출시</td>\n",
       "      <td>[농협은행은행장 권준학은 지난 3월 그린뉴딜 정책방향에 맞춰 친환경 경영 우수기업에...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>한국일보</td>\n",
       "      <td>팩 친환경 포장재 개발 · · · 제품 전체에 적용</td>\n",
       "      <td>[그룹회장 허영인이 기업의 지속 가능한 성장을 추구하는 환경 사회 지배구조 경영 강...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>한국일보</td>\n",
       "      <td>친환경 최고층 모범··· 롯데월드타워 경영 강화</td>\n",
       "      <td>[롯데월드타워는 설계 단계에서부터 친환경 에너지 생산과 효율적인 관리를 목표로 만들...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>한국일보</td>\n",
       "      <td>年 2회 호텔 친환경 심사··· 농가 상생 프로젝트</td>\n",
       "      <td>[지난 7일 국내 호텔업계 맏형으로 손꼽히는 롯데호텔이 지속가능 경영 강화를 위해 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>한국일보</td>\n",
       "      <td>빨대 없는 컵커피 등 친환경 상품 라인업 강화</td>\n",
       "      <td>[편의점 세븐일레븐은 올해 초 경영 선포 이후 친환경 상품 라인업을 본격적으로 강화...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      언론사                                               제목  \\\n",
       "0    경향신문                    LG화학, 2조6000억 투자해 친환경 소재 육성한다   \n",
       "1    경향신문  친환경차 패권 의지 드러낸 미국···한국차 미·중·EU ‘신 경제블록’ 진입 모색해야   \n",
       "2    경향신문       중소형 선박에 LPG엔진을···“친환경 시장 주도” 부산 전국 첫 실증 착수   \n",
       "3    경향신문             비정유·친환경 미래 먹거리 찾아나서는 정유업계, ‘탈석유’ 잰걸음   \n",
       "4    경향신문                   화학업계의 친환경 행보? '플라스틱 생산량부터 줄여야'   \n",
       "..    ...                                              ...   \n",
       "501  한국일보                             실천 기업 우대 친환경기업우대론 출시   \n",
       "502  한국일보                     팩 친환경 포장재 개발 · · · 제품 전체에 적용   \n",
       "503  한국일보                       친환경 최고층 모범··· 롯데월드타워 경영 강화   \n",
       "504  한국일보                     年 2회 호텔 친환경 심사··· 농가 상생 프로젝트   \n",
       "505  한국일보                        빨대 없는 컵커피 등 친환경 상품 라인업 강화   \n",
       "\n",
       "                                                    내용  \n",
       "0    화학 대산공장 전경. 화학 제공. 화학이 친환경 소재 사업을 육성하기 위해 대규모 ...  \n",
       "1    조 바이든 미국 대통령이 지난 5일현지시간 미국 워싱턴 백악관에서 열린 무공해차 홍...  \n",
       "2    부산시가 중·소형선박 엔진을 액화석유가스 엔진으로 전환하기 위한 실증작업에 나선다....  \n",
       "3    현대오일뱅크는 2030년까지 친환경미래사업의 영업이익 비중을 70까지 확대하기로 했...  \n",
       "4    롯데케미칼 소재 제품 포장백. 롯데케미칼 제공. 과거에는 환경오염의 주범으로 꼽혔던...  \n",
       "..                                                 ...  \n",
       "501  [농협은행은행장 권준학은 지난 3월 그린뉴딜 정책방향에 맞춰 친환경 경영 우수기업에...  \n",
       "502  [그룹회장 허영인이 기업의 지속 가능한 성장을 추구하는 환경 사회 지배구조 경영 강...  \n",
       "503  [롯데월드타워는 설계 단계에서부터 친환경 에너지 생산과 효율적인 관리를 목표로 만들...  \n",
       "504  [지난 7일 국내 호텔업계 맏형으로 손꼽히는 롯데호텔이 지속가능 경영 강화를 위해 ...  \n",
       "505  [편의점 세븐일레븐은 올해 초 경영 선포 이후 친환경 상품 라인업을 본격적으로 강화...  \n",
       "\n",
       "[506 rows x 3 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df=news_df.drop(['제목'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df=news_df.drop(['언론사'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>내용</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>화학 대산공장 전경. 화학 제공. 화학이 친환경 소재 사업을 육성하기 위해 대규모 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>조 바이든 미국 대통령이 지난 5일현지시간 미국 워싱턴 백악관에서 열린 무공해차 홍...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>부산시가 중·소형선박 엔진을 액화석유가스 엔진으로 전환하기 위한 실증작업에 나선다....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>현대오일뱅크는 2030년까지 친환경미래사업의 영업이익 비중을 70까지 확대하기로 했...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>롯데케미칼 소재 제품 포장백. 롯데케미칼 제공. 과거에는 환경오염의 주범으로 꼽혔던...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>[농협은행은행장 권준학은 지난 3월 그린뉴딜 정책방향에 맞춰 친환경 경영 우수기업에...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>[그룹회장 허영인이 기업의 지속 가능한 성장을 추구하는 환경 사회 지배구조 경영 강...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>[롯데월드타워는 설계 단계에서부터 친환경 에너지 생산과 효율적인 관리를 목표로 만들...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>[지난 7일 국내 호텔업계 맏형으로 손꼽히는 롯데호텔이 지속가능 경영 강화를 위해 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>[편의점 세븐일레븐은 올해 초 경영 선포 이후 친환경 상품 라인업을 본격적으로 강화...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    내용\n",
       "0    화학 대산공장 전경. 화학 제공. 화학이 친환경 소재 사업을 육성하기 위해 대규모 ...\n",
       "1    조 바이든 미국 대통령이 지난 5일현지시간 미국 워싱턴 백악관에서 열린 무공해차 홍...\n",
       "2    부산시가 중·소형선박 엔진을 액화석유가스 엔진으로 전환하기 위한 실증작업에 나선다....\n",
       "3    현대오일뱅크는 2030년까지 친환경미래사업의 영업이익 비중을 70까지 확대하기로 했...\n",
       "4    롯데케미칼 소재 제품 포장백. 롯데케미칼 제공. 과거에는 환경오염의 주범으로 꼽혔던...\n",
       "..                                                 ...\n",
       "501  [농협은행은행장 권준학은 지난 3월 그린뉴딜 정책방향에 맞춰 친환경 경영 우수기업에...\n",
       "502  [그룹회장 허영인이 기업의 지속 가능한 성장을 추구하는 환경 사회 지배구조 경영 강...\n",
       "503  [롯데월드타워는 설계 단계에서부터 친환경 에너지 생산과 효율적인 관리를 목표로 만들...\n",
       "504  [지난 7일 국내 호텔업계 맏형으로 손꼽히는 롯데호텔이 지속가능 경영 강화를 위해 ...\n",
       "505  [편의점 세븐일레븐은 올해 초 경영 선포 이후 친환경 상품 라인업을 본격적으로 강화...\n",
       "\n",
       "[506 rows x 1 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    cleaned_text = re.sub('[^ ㄱ-ㅣ가-힣]+','.',text) # 한국어, 스페이스바, .를 제외한 모든 문자 제거\n",
    "    cleaned_text = ' '.join(cleaned_text.split()) # 너무 많은 공백이 생기므로 공백 1개로\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df['내용']=news_df['내용'].apply(lambda x:clean_text(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.to_csv('친환경.csv', encoding='CP949')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
