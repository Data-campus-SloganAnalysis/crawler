{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 크롤러 환경설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import itertools\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"headless\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각자 chromedriver.exe 경로 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_path = 'C:/Users/chaeh/chromedriver.exe'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특수문자 및 영어등 텍스트 전처리 함수 -> clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    cleaned_text = re.sub('[a-zA-Z]', '', text)\n",
    "    cleaned_text = re.sub('[\\{\\}\\[\\]\\/?,;:|\\)*~`!^\\-_+<>@\\#$%&\\\\\\=\\(\\'\\\"]',\n",
    "                          '', cleaned_text)\n",
    "    cleaned_text = ' '.join(cleaned_text.split())\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 경향신문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#각 네이버 일간지 기사 크롤링한 부분을 함수로 표현\n",
    "def biz_khan(query,driver_path,page):\n",
    "    \n",
    "    head_list = [] #기사 제목\n",
    "    data_list = [] #기사 내용\n",
    "    \n",
    "    current_page = 1 #현재 페이지 1로 설정(1부터 page까지 크롤링)\n",
    "    last_page =(int(page)-1)*10+1 \n",
    "    \n",
    "    domain = '경향신문' #네이버뉴스에서 어떤 일간지 기사 가져올건지 도메인 설정(html)\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        \n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "        \n",
    "        # 달라지는 url 기준으로 변수설정해서 url 가져오기\n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1032&nso=&is_sug_officeid=0' \n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        # 한페이지에 있는 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "               \n",
    "        except: #기사가 없을 경우\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "        \n",
    "        # url_list안에 있는 모든 기사들 크롤링\n",
    "        for url in url_list:\n",
    "            \n",
    "            #url_list에 None이 있을경우\n",
    "            if url=='None':\n",
    "                continue\n",
    "            \n",
    "            driver = webdriver.Chrome(executable_path=driver_path, options=options) \n",
    "            time.sleep(2) #로딩되는 시간 설정(2초)\n",
    "            driver.get(url)\n",
    "            time.sleep(2)\n",
    "            \n",
    "            # 각 언론사의 html 구조에 맞게 기사 제목과 내용 크롤링\n",
    "            try:\n",
    "                soup.find('h1',id='articleTtitle')\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(soup.find('h1',id='articleTtitle').text)\n",
    "                tags = soup.select_one('#container > div.main_container > div.art_cont > div.art_body').find_all('p')\n",
    "                data = [clean_text(x.text) for x in tags]\n",
    "                data_list.append(' '.join(data))\n",
    "                \n",
    "            except: # html 구조가 다를경우\n",
    "                try:\n",
    "                    soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                    head_list.append(soup.find('h1',id='article_title').text)\n",
    "                    tags = soup.select_one('#articleBody').find_all('p')\n",
    "                    data = [clean_text(x.text) for x in tags]\n",
    "                    data_list.append(' '.join(data))\n",
    "                    \n",
    "                except: # html구조가 오류났을 경우\n",
    "                    print('None_type')\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "\n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 국민일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmib(query,driver_path,page):\n",
    "    \n",
    "    head_list = [] \n",
    "    data_list = [] \n",
    "\n",
    "    current_page = 1 \n",
    "    last_page =(int(page)-1)*10+1 \n",
    "\n",
    "    domain = '국민일보' \n",
    "    \n",
    "    while current_page < last_page:\n",
    "        \n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "        \n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1005&nso=so%3Ar%2Cp%3Aall&is_sug_officeid=0' \n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "                \n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "        \n",
    "        for url in url_list:\n",
    "            \n",
    "            if url=='None':\n",
    "                continue\n",
    "                \n",
    "            driver = webdriver.Chrome(executable_path=driver_path,options=options) \n",
    "            time.sleep(2)\n",
    "            driver.get(url)\n",
    "            time.sleep(2)\n",
    " \n",
    "            try:\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(soup.select_one('#sub > div.sub_header > div > div.nwsti > h3').text)\n",
    "                tags = soup.select_one(\"#articleBody\").text\n",
    "                data=tags.strip()\n",
    "                data=data.strip('\\t')\n",
    "                data=clean_text(data)\n",
    "                data_list.append(data)\n",
    "                \n",
    "            except:\n",
    "                print('None_type')\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "             \n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 내일신문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naeil(query,driver_path,page):\n",
    "    \n",
    "    data_list = []\n",
    "    head_list = []\n",
    "    \n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    domain = '내일신문'\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        \n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "        \n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2312&nso=&is_sug_officeid=0' \n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "                \n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "        for url in url_list:\n",
    "            \n",
    "            if url=='None':\n",
    "                continue\n",
    "                \n",
    "            driver = webdriver.Chrome(executable_path=driver_path,options=options) \n",
    "            time.sleep(2)\n",
    "            driver.get(url)\n",
    "            time.sleep(2)\n",
    "\n",
    "            try:\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(soup.select_one('#contentArea > div.caL2 > div > div.articleArea > h3').text)\n",
    "                tags = soup.select_one(\"#contents\").text\n",
    "                data=tags.strip()\n",
    "                data=data.strip('\\t')\n",
    "                data=clean_text(data)\n",
    "                data_list.append(data)\n",
    "                \n",
    "            except:\n",
    "                print('None_type')\n",
    "            driver.close()\n",
    "        current_page+=10  \n",
    "        \n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 동아일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def donga(query,driver_path, page):\n",
    "    \n",
    "    data_list = []\n",
    "    head_list = []\n",
    "    \n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    domain = '동아일보'\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        \n",
    "        driver =webdriver.Chrome(executable_path =driver_path, options=options)\n",
    "        \n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1020&nso=&is_sug_officeid=0'\n",
    "        url = url_domain + query + '&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    " \n",
    "   \n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "    \n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "            \n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "    \n",
    "    \n",
    "        for url in url_list:\n",
    "            \n",
    "            if url=='None':\n",
    "                continue\n",
    "                \n",
    "            driver = webdriver.Chrome(executable_path=driver_path,options=options)\n",
    "        \n",
    "            try:\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(soup.select_one('#container > div.article_title > h1').text)\n",
    "                tags = soup.select_one('#content > div > div.article_txt').text\n",
    "                tags=clean_text(tags)\n",
    "                data_list.append(tags)\n",
    "                \n",
    "            except:\n",
    "                try:\n",
    "                    soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                    head_list.append(soup.select_one('#content > div.article_title > h2').text)\n",
    "                    tags = soup.select_one('#ct').text\n",
    "                    tags=clean_text(tags)\n",
    "                    data_list.append(tags)\n",
    "                    \n",
    "                except:\n",
    "                    print('None_type')\n",
    "            driver.close()\n",
    "        current_page += 10\n",
    "        \n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 매일일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_i(query,driver_path,page):\n",
    "    \n",
    "    data_list = []\n",
    "    head_list = []\n",
    "    \n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    domain = '매일일보'\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        \n",
    "        driver = webdriver.Chrome(executable_path =driver_path, options=options)\n",
    "        \n",
    "        url_domain ='https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2385&nso=so%3Ar%2Cp%3Aall%2Ca%3Aall&is_sug_officeid=0'\n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "    \n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "    \n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "    \n",
    "        except: \n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return ()\n",
    "        driver.close()\n",
    "    \n",
    "        for url in url_list:\n",
    "            \n",
    "            if url=='None':\n",
    "                continue\n",
    "                \n",
    "            driver = webdriver.Chrome(executable_path=driver_path,options=options) \n",
    "            \n",
    "            try:\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(soup.select('#user-container > div.custom-pc.float-center.max-width-1130 > header > div > div')[0].text)\n",
    "                tags = soup.find(id='article-view-content-div').find_all('p')\n",
    "                data = [clean_text(x.text) for x in tags]\n",
    "                data_list.append(' '.join(data))\n",
    "                \n",
    "            except:\n",
    "                print('None_type')\n",
    "            driver.close()\n",
    "        current_page += 10\n",
    "        \n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문화일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def munhwa(query,driver_path,page):\n",
    "     \n",
    "    data_list = []\n",
    "    head_list = []\n",
    "    \n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    domain = '문화일보'\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        \n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "        \n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain = '%EB%89%B4%EC%8A%A4&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1021&nso=&is_sug_officeid=0'\n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "        \n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            \n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "                \n",
    "        except:\n",
    "            print('기사가 없습니다')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "        \n",
    "        for url in url_list:\n",
    "            \n",
    "            if url=='None':\n",
    "                continue\n",
    "    \n",
    "            driver = webdriver.Chrome(executable_path=driver_path,options=options) \n",
    "                \n",
    "            try:\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(soup.find('span', class_ = 'title').text)\n",
    "                data = soup.select('#NewsAdContent')[0].text\n",
    "                data=data.strip()\n",
    "                data=data.strip('\\t')\n",
    "                data=clean_text(data)\n",
    "                data_list.append(data)\n",
    "            \n",
    "            except:\n",
    "                try:\n",
    "                    soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                    head_list.append(soup.find('h3', id = 'articleTitle').text)\n",
    "                    data = soup.select('#articleBodyContents')[0].text\n",
    "                    data=data.strip()\n",
    "                    data=data.strip('\\t')\n",
    "                    data=clean_text(data)\n",
    "                    data_list.append(data)\n",
    "                    \n",
    "                except:\n",
    "                    print(\"None_type\")\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "        \n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 서울신문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seoul(query,driver_path,page):\n",
    "    \n",
    "    data_list = []\n",
    "    head_list = []\n",
    "    \n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    domain = '서울신문'\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        \n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "        \n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='%EB%89%B4%EC%8A%A4&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1081&nso=&is_sug_officeid=0' \n",
    "        url = url_domain + query + '&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "        \n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "                \n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "        \n",
    "        for url in url_list:\n",
    "            \n",
    "            if url=='None':\n",
    "                continue\n",
    "                \n",
    "            driver = webdriver.Chrome(executable_path=driver_path,options=options) \n",
    "            time.sleep(2)\n",
    "            driver.get(url)\n",
    "            time.sleep(2)\n",
    "            \n",
    "            try:\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(soup.select_one('#viewWrapDiv > div.S20_title > div.S20_article_tit > h1').text)\n",
    "                tags = soup.select_one(\"#atic_txt1\").text\n",
    "                data=tags.strip()\n",
    "                data=data.strip('\\t')\n",
    "                data=clean_text(data)\n",
    "                data_list.append(data)\n",
    "                \n",
    "            except:\n",
    "                print('None_type')\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "             \n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 세계일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segye(query,driver_path,page):\n",
    "    \n",
    "    data_list = []\n",
    "    head_list = []\n",
    "    \n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    domain = '세계일보'\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        \n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "        \n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1022&nso=&is_sug_officeid=0'\n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page)+ news_domain\n",
    "        driver.get(url)\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "                \n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "        for url in url_list:\n",
    "            \n",
    "            if url=='None':\n",
    "                continue\n",
    "                \n",
    "            driver = webdriver.Chrome(executable_path=driver_path) \n",
    "\n",
    "            try:\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(soup.select('#title_sns')[0].text)\n",
    "                data = soup.find('article',class_='viewBox').text\n",
    "                data=data.strip()\n",
    "                data=clean_text(data)\n",
    "                data_list.append(data)\n",
    "                \n",
    "            except:\n",
    "                print('None_type')\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "        \n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 아시아투데이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asiatoday(query,driver_path,page):\n",
    "      \n",
    "    data_list = []\n",
    "    head_list = []\n",
    "    \n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    domain = '아시아투데이'\n",
    "    \n",
    "    \n",
    "    while current_page < last_page:\n",
    "        \n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "        \n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2268&nso=&is_sug_officeid=0'\n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > sec6tion > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "                \n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "        for url in url_list:\n",
    "            \n",
    "            if url=='None':\n",
    "                continue\n",
    "                \n",
    "            driver = webdriver.Chrome(executable_path=driver_path) \n",
    "\n",
    "            try:\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(soup.select('#section_top > div > h3')[0].text)\n",
    "                data = soup.find('div',class_='news_bm').text\n",
    "                data=data.strip()\n",
    "                data=clean_text(data)\n",
    "                data_list.append(data)\n",
    "                \n",
    "            except:\n",
    "                print('None_type')\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "        \n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전국매일신문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alldays(query,driver_path,page):\n",
    "    \n",
    "    data_list = []\n",
    "    head_list = []\n",
    "    \n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    domain = '전국매일신문'\n",
    "    \n",
    "    while current_page < last_page:      \n",
    "        \n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options) \n",
    "        \n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain = '&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2844&nso=&is_sug_officeid=0'\n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "                \n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "        for url in url_list:\n",
    "            \n",
    "            if url=='None':\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                driver = webdriver.Chrome(executable_path=driver_path, options=options) \n",
    "                driver.get(url)\n",
    "                time.sleep(2)\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head = clean_text(soup.find('div',class_=\"article-head-title\").text)\n",
    "                head_list.append(head)\n",
    "                tags = soup.select_one('#article-view-content-div').find_all('p')\n",
    "                data = [clean_text(x.text) for x in tags]\n",
    "                data_list.append(' '.join(data))\n",
    "                \n",
    "            except:\n",
    "                print(\"None_type\")\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "        \n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                           '제목' :  head_list,\n",
    "                           '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 조선일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joseon(query,driver_path,page):\n",
    "    \n",
    "    data_list = []\n",
    "    head_list = []\n",
    "    \n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    domain = '조선일보'\n",
    "    \n",
    "    while current_page < last_page:  \n",
    "        \n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "        \n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain = '&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1023&nso=&is_sug_officeid=0'\n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "                \n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "        for url in url_list:\n",
    "            \n",
    "            if url=='None':\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "                driver.get(url)\n",
    "                time.sleep(2)\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head = clean_text(soup.find_all(\"h1\", class_=\"article-header__headline | font--primary text--black\")[0].text)\n",
    "                head_list.append(head)\n",
    "                tags = soup.find_all('p',class_=' article-body__content article-body__content-text | text--black text font--size-sm-18 font--size-md-18 font--primary')\n",
    "                data = [clean_text(x.text) for x in tags]\n",
    "                data_list.append(' '.join(data))\n",
    "                \n",
    "            except:\n",
    "                print(\"None_type\")\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "        \n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                           '제목' :  head_list,\n",
    "                           '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 중앙일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chungang(query,driver_path,page):\n",
    "    \n",
    "    data_list = []\n",
    "    head_list = []\n",
    "    \n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    domain = '중앙일보'\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        \n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options) \n",
    "        \n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain = '&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1025&nso=&is_sug_officeid=0'\n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "                \n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "        for url in url_list:\n",
    "            \n",
    "            if url=='None':\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                driver = webdriver.Chrome(executable_path=driver_path, options=options) \n",
    "                time.sleep(2)\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head = clean_text(soup.select('#article_title')[0].text)\n",
    "                head_list.append(head)\n",
    "                text = soup.find('div', id='article_body').text\n",
    "                text = clean_text(text)\n",
    "                data_list.append(text)\n",
    "                \n",
    "            except:\n",
    "                print(\"None_type\")\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "        \n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                           '제목' :  head_list,\n",
    "                           '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 천지일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newscj(query,driver_path,page):\n",
    "       \n",
    "    data_list = []\n",
    "    head_list = []\n",
    "    \n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    domain = '천지일보'\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        \n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options) \n",
    "        \n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2041&nso=&is_sug_officeid=0' \n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "                \n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "        for url in url_list:\n",
    "            \n",
    "            if url=='None':\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                driver = webdriver.Chrome(executable_path=driver_path, options=options) \n",
    "                driver.get(url)\n",
    "                time.sleep(2)\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head = clean_text(soup.find('div',class_=\"article-head-title\").text)\n",
    "                head_list.append(head)\n",
    "                tags = soup.select_one('#article-view-content-div').find_all('p')\n",
    "                data = [clean_text(x.text) for x in tags]\n",
    "                data_list.append(' '.join(data))\n",
    "                \n",
    "            except:\n",
    "                print(\"None_type\")\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "        \n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                           '제목' :  head_list,\n",
    "                           '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한겨례"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hani(query,driver_path,page):\n",
    "    \n",
    "    data_list = []\n",
    "    head_list = []\n",
    "    \n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    domain = '한겨례'\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        \n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options) \n",
    "        \n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1028&nso=&is_sug_officeid=0' \n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "                \n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "        for url in url_list:\n",
    "            \n",
    "            if url=='None':\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                driver = webdriver.Chrome(executable_path=driver_path, options=options)  \n",
    "                driver.get(url)\n",
    "                time.sleep(2)\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head = clean_text(soup.find('span', class_='title').text)\n",
    "                head_list.append(head)\n",
    "                tags = soup.select_one('#a-left-scroll-in > div.article-text > div').find_all('div', class_='text')\n",
    "                data = [clean_text(x.text) for x in tags]\n",
    "                data_list.append(' '.join(data))\n",
    "                \n",
    "            except:\n",
    "                print(\"None_type\")\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "        \n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                           '제목' :  head_list,\n",
    "                           '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한국일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hankook(query,driver_path,page):\n",
    "       \n",
    "    data_list = []\n",
    "    head_list = []\n",
    "    \n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    domain = '한국일보'\n",
    "\n",
    "    while current_page < last_page:\n",
    "        \n",
    "        driver = webdriver.Chrome(executable_path = driver_path,options=options)\n",
    "        \n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain = '%EB%89%B4%EC%8A%A4&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1469&nso=&is_sug_officeid=0'\n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "        \n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "                \n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "        for url in url_list:\n",
    "            \n",
    "            if url=='None':\n",
    "                continue\n",
    "                \n",
    "            driver = webdriver.Chrome(executable_path=driver_path, options=options)  # for Windows\n",
    "            driver.get(url)\n",
    "            time.sleep(2)\n",
    "\n",
    "            try:\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(clean_text(soup.find('h2').text))\n",
    "                tags = soup.select_one('body > div.wrap > div.container.end.end-uni > div.end-body > div > div.col-main.read').find_all('p')\n",
    "                data = [clean_text(x.text) for x in tags]\n",
    "                data_list.append(data)\n",
    "                \n",
    "            except:\n",
    "                print('None_type')\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "        \n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 원하는 keyword와 페이지 수 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 네이버 뉴스 모든 일간지 이름 list (dataframe 합칠때 None인 데이터들은 구별해주기 위한 작업)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawling_news(query,max_page):\n",
    "    biz_df = biz_khan(query,driver_path,max_page)\n",
    "    kmib_df = kmib(query,driver_path,max_page)\n",
    "    naeil_df = naeil(query,driver_path,max_page)\n",
    "    donga_df = donga(query,driver_path,max_page)\n",
    "    mi_df = m_i(query,driver_path,max_page)\n",
    "    munhwa_df = munhwa(query,driver_path,max_page)\n",
    "    seoul_df = seoul(query,driver_path,max_page)\n",
    "    segye_df = segye(query,driver_path,max_page)\n",
    "    asia_df = asiatoday(query,driver_path,max_page)\n",
    "    alldays_df = alldays(query, driver_path,max_page)\n",
    "    joseon_df = joseon(query, driver_path,max_page)\n",
    "    chungang_df = chungang(query, driver_path,max_page)\n",
    "    cj_df = newscj(query,driver_path,max_page)\n",
    "    hani_df = hani(query,driver_path,max_page)\n",
    "    hk_df = hankook(query,driver_path,max_page)\n",
    "    \n",
    "    #dataframe 합칠때 None인 데이터들은 구별해주기 위해 list만듬\n",
    "    df_list=[biz_df, kmib_df, naeil_df, donga_df, mi_df, munhwa_df, seoul_df, segye_df, asia_df, alldays_df ,joseon_df, chungang_df, cj_df, hani_df, hk_df]\n",
    "    \n",
    "    #dataframe안이 str형일때만 apoend\n",
    "    names=[]\n",
    "    for name in df_list:\n",
    "        \n",
    "        if type(name)!=int:\n",
    "            names.append(name)\n",
    "    \n",
    "    #각각의 news df 합치기, 제목중복되는것들은 drop시켜주고, 제목과 언론사는 필요없으니 drop\n",
    "    news_df=pd.concat(names)\n",
    "    news_df.drop_duplicates(['제목'],inplace=True,ignore_index=True)\n",
    "    news_df=news_df.drop(['제목'],axis=1)\n",
    "    news_df=news_df.drop(['언론사'],axis=1)\n",
    "    \n",
    "    return news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨링할 문장 split하는 함수\n",
    "def create_news_df_split(df_len):\n",
    "    \n",
    "    text=[]\n",
    "    count=0 #의미없는변수 (except를 위해만듬)\n",
    "    \n",
    "    for x in range(0,df_len):\n",
    "        \n",
    "        try: #'다'를 기준으로 문장을 나눔\n",
    "            tp=news_df['내용'][x]\n",
    "            tp=tp.split('다.')\n",
    "            tp = [x+'다' for x in tp]\n",
    "            tp=clean_text(tp)\n",
    "            print(tp)\n",
    "            \n",
    "        except:\n",
    "            count+=1\n",
    "            \n",
    "        text.append(tp)\n",
    "    text=list(itertools.chain(*text))\n",
    "    \n",
    "    df = pd.DataFrame([x for x in text])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df=crawling_news('친환경',5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_len=len(news_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>다음달 독일 뮌휀에서 열리는 ‘ 모빌리티 2021’에 마련될 현대모비스 전시관조감도...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>세계 4대 모터쇼 중 하나로 프랑크푸르트 모터쇼로도 잘 알려진 모빌리티는 유럽 최...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>홀수 해마다 프랑크푸르트에서 열리다 올해부터는 뮌헨으로 옮겨 개최한다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>현대모비스가 모빌리티에 참가하기는 처음이다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>올해 행사에는 국내 자동차부품 업체로는 현대모비스가 유일하게 참가하는 것으로 알려졌다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1379</th>\n",
       "      <td>탈출에 목숨까지 거는데...아프간 추락 형제 조롱한 티셔츠 팔다니</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1380</th>\n",
       "      <td>야권 대선 경쟁 본격화</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1381</th>\n",
       "      <td>정치인생 10년 열 번째 철수...아련해진 안철수의 새 정치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1382</th>\n",
       "      <td>코로나19 4차 유행</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1383</th>\n",
       "      <td>오늘부터 수도권 식당·카페 오후 9시까지...백신접종 인센티브 적용</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1384 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0\n",
       "0     다음달 독일 뮌휀에서 열리는 ‘ 모빌리티 2021’에 마련될 현대모비스 전시관조감도...\n",
       "1      세계 4대 모터쇼 중 하나로 프랑크푸르트 모터쇼로도 잘 알려진 모빌리티는 유럽 최...\n",
       "2                홀수 해마다 프랑크푸르트에서 열리다 올해부터는 뮌헨으로 옮겨 개최한다\n",
       "3                               현대모비스가 모빌리티에 참가하기는 처음이다\n",
       "4       올해 행사에는 국내 자동차부품 업체로는 현대모비스가 유일하게 참가하는 것으로 알려졌다\n",
       "...                                                 ...\n",
       "1379               탈출에 목숨까지 거는데...아프간 추락 형제 조롱한 티셔츠 팔다니\n",
       "1380                                       야권 대선 경쟁 본격화\n",
       "1381                  정치인생 10년 열 번째 철수...아련해진 안철수의 새 정치\n",
       "1382                                        코로나19 4차 유행\n",
       "1383              오늘부터 수도권 식당·카페 오후 9시까지...백신접종 인센티브 적용\n",
       "\n",
       "[1384 rows x 1 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_news_df_split(df_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.to_csv('친환경.csv', encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
