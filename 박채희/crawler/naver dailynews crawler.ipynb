{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 크롤러 환경설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"headless\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각자 chromedriver.exe 경로 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_path = 'C:/Users/chaeh/chromedriver.exe'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특수문자 및 영어등 텍스트 전처리 함수 -> clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    cleaned_text = re.sub('[a-zA-Z]', '', text)\n",
    "    cleaned_text = re.sub('[\\{\\}\\[\\]\\/?,;:|\\)*~`!^\\-_+<>@\\#$%&\\\\\\=\\(\\'\\\"]',\n",
    "                          '', cleaned_text)\n",
    "    cleaned_text = ' '.join(cleaned_text.split())\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 경향신문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biz_khan(query,driver_path,page):\n",
    "    head_list = []\n",
    "    data_list = []\n",
    "    current_page = 1 \n",
    "    last_page =(int(page)-1)*10+1\n",
    "    domain = '경향신문'\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options) \n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1032&nso=&is_sug_officeid=0' \n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "        for url in url_list:\n",
    "            driver = webdriver.Chrome(executable_path=driver_path, options=options)  # for Windows\n",
    "            driver.get(url)\n",
    "            time.sleep(2)\n",
    "            # try문 안에서 각자 언론사에 맞게 수정\n",
    "            try:\n",
    "                soup.find('h1',id='articleTtitle')\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(soup.find('h1',id='articleTtitle').text)\n",
    "                tags = soup.select_one('#container > div.main_container > div.art_cont > div.art_body').find_all('p')\n",
    "                data = [clean_text(x.text) for x in tags]\n",
    "                data_list.append(' '.join(data))\n",
    "            except:\n",
    "                try:\n",
    "                    soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                    head_list.append(soup.find('h1',id='article_title').text)\n",
    "                    tags = soup.select_one('#articleBody').find_all('p')\n",
    "                    data = [clean_text(x.text) for x in tags]\n",
    "                    data_list.append(' '.join(data))\n",
    "                except:\n",
    "                    print(url)\n",
    "                    print('None_type')\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "\n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 국민일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmib(query,driver_path,page):\n",
    "    domain = '국민일보'\n",
    "    driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    data_list = []\n",
    "    head_list = []\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1005&nso=so%3Ar%2Cp%3Aall&is_sug_officeid=0' \n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        #print(url)\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "        \n",
    "        for url in url_list:\n",
    "            driver = webdriver.Chrome(executable_path=driver_path,options=options)  # for Windows\n",
    "            time.sleep(2)\n",
    "            driver.get(url)\n",
    "            time.sleep(2)\n",
    "            # try문 안에서 각자 언론사에 맞게 수정\n",
    "            try:\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(soup.select_one('#sub > div.sub_header > div > div.nwsti > h3').text)\n",
    "                tags = soup.select_one(\"#articleBody\").text\n",
    "                data=tags.strip()\n",
    "                data=data.strip('\\t')\n",
    "                data=clean_text(data)\n",
    "                data_list.append(data)\n",
    "            except:\n",
    "                print(url)\n",
    "                print('None_type')\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "             \n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 내일신문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naeil(query,driver_path,page):\n",
    "    domain = '내일신문'\n",
    "    driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    data_list = []\n",
    "    head_list = []\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2312&nso=&is_sug_officeid=0' \n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        #print(url)\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "        for url in url_list:\n",
    "            driver = webdriver.Chrome(executable_path=driver_path,options=options)  # for Windows\n",
    "            time.sleep(2)\n",
    "            driver.get(url)\n",
    "            time.sleep(2)\n",
    "            # try문 안에서 각자 언론사에 맞게 수정\n",
    "            try:\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(soup.select_one('#contentArea > div.caL2 > div > div.articleArea > h3').text)\n",
    "                tags = soup.select_one(\"#contents\").text\n",
    "                data=tags.strip()\n",
    "                data=data.strip('\\t')\n",
    "                data=clean_text(data)\n",
    "                data_list.append(data)\n",
    "            except:\n",
    "                print(url)\n",
    "                print('None_type')\n",
    "            driver.close()\n",
    "        current_page+=10  \n",
    "        \n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 동아일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def donga(query,driver_path, page):\n",
    "    domain = '동아일보'\n",
    "    driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "    \n",
    "    current_page =1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    data_list =[]\n",
    "    head_list=[]\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver =webdriver.Chrome(executable_path =driver_path, options =options)\n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1020&nso=&is_sug_officeid=0'\n",
    "        url = url_domain + query + '&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    " \n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "    \n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "            \n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "    \n",
    "    \n",
    "        for url in url_list:\n",
    "            driver = webdriver.Chrome(executable_path=driver_path,options=options)\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                time.sleep(1)\n",
    "            except:\n",
    "                print('link error')\n",
    "                continue\n",
    "        # try문 안에서 각자 언론사에 맞게 수정\n",
    "            try:\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(soup.select_one('#container > div.article_title > h1').text)\n",
    "                tags = soup.select_one('#content > div > div.article_txt').text\n",
    "                tags=clean_text(tags)\n",
    "                data_list.append(tags)\n",
    "            except:\n",
    "                try:\n",
    "                    soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                    head_list.append(soup.select_one('#content > div.article_title > h2').text)\n",
    "                    tags = soup.select_one('#ct').text\n",
    "                    tags=clean_text(tags)\n",
    "                    data_list.append(tags)\n",
    "                except:\n",
    "                    print(url)\n",
    "                    print('None_type')\n",
    "            driver.close()\n",
    "        current_page += 10\n",
    "        \n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 매일일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_i(query,driver_path,page):\n",
    "    domain = '매일일보'\n",
    "    driver = webdriver.Chrome(executable_path=driver_path, options=options) \n",
    "    \n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    data_list = []\n",
    "    head_list = []\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path =driver_path, options=options)\n",
    "        url_domain ='https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2385&nso=so%3Ar%2Cp%3Aall%2Ca%3Aall&is_sug_officeid=0'\n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        print(url)\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "    \n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "    \n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "    \n",
    "        except: \n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return ()\n",
    "        driver.close()\n",
    "    \n",
    "        for url in url_list:\n",
    "            driver = webdriver.Chrome(executable_path=driver_path,options=options) \n",
    "            \n",
    "            try:\n",
    "                driver.get(url)\n",
    "                time.sleep(1)\n",
    "            except:\n",
    "                print('link error')\n",
    "                continue\n",
    "        # try문 안에서 각자 언론사에 맞게 수정\n",
    "            try:\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(soup.select('#user-container > div.custom-pc.float-center.max-width-1130 > header > div > div')[0].text)\n",
    "                tags = soup.find(id='article-view-content-div').find_all('p')\n",
    "                data = [clean_text(x.text) for x in tags]\n",
    "                data_list.append(' '.join(data))\n",
    "            except:\n",
    "                print(url)\n",
    "                print('None_type')\n",
    "            driver.close()\n",
    "        current_page += 10\n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문화일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def munhwa(query,driver_path,page):\n",
    "    domain = '문화일보'\n",
    "    \n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    head_list = []\n",
    "    data_list = []\n",
    "    \n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain = '%EB%89%B4%EC%8A%A4&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1021&nso=&is_sug_officeid=0'\n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "        \n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            \n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "                \n",
    "        except:\n",
    "            print('기사가 없습니다')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "        \n",
    "        \n",
    "        for url in url_list:\n",
    "            driver = webdriver.Chrome(executable_path=driver_path)  # for Windows\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                time.sleep(1)\n",
    "            \n",
    "            except:\n",
    "                print('link error')\n",
    "                continue\n",
    "                # try문 안에서 각자 언론사에 맞게 수정\n",
    "                \n",
    "            try:\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(soup.find('span', class_ = 'title').text)\n",
    "                data = soup.select('#NewsAdContent')[0].text\n",
    "                data=data.strip()\n",
    "                data=data.strip('\\t')\n",
    "                data=clean_text(data)\n",
    "                data_list.append(data)\n",
    "            \n",
    "            except:\n",
    "                try:\n",
    "                    soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                    head_list.append(soup.find('h3', id = 'articleTitle').text)\n",
    "                    data = soup.select('#articleBodyContents')[0].text\n",
    "                    data=data.strip()\n",
    "                    data=data.strip('\\t')\n",
    "                    data=clean_text(data)\n",
    "                    data_list.append(data)\n",
    "                except:\n",
    "                    print(url)\n",
    "                    print(\"None_type\")\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "        \n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 서울신문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seoul(query,driver_path,page):\n",
    "    domain = '서울신문'\n",
    "    driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    data_list = []\n",
    "    head_list = []\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='%EB%89%B4%EC%8A%A4&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1081&nso=&is_sug_officeid=0' \n",
    "        url = url_domain + query + '&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "        \n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "        \n",
    "        for url in url_list:\n",
    "            driver = webdriver.Chrome(executable_path=driver_path,options=options)  # for Windows\n",
    "            time.sleep(2)\n",
    "            driver.get(url)\n",
    "            time.sleep(2)\n",
    "            # try문 안에서 각자 언론사에 맞게 수정\n",
    "            try:\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(soup.select_one('#viewWrapDiv > div.S20_title > div.S20_article_tit > h1').text)\n",
    "                tags = soup.select_one(\"#atic_txt1\").text\n",
    "                data=tags.strip()\n",
    "                data=data.strip('\\t')\n",
    "                data=clean_text(data)\n",
    "                data_list.append(data)\n",
    "            except:\n",
    "                print(url)\n",
    "                print('None_type')\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "             \n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 세계일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segye(query,driver_path,page):\n",
    "    domain = '세계일보'\n",
    "    driver = webdriver.Chrome(executable_path=driver_path)\n",
    "        \n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    head_list = []\n",
    "    data_list = []\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1022&nso=&is_sug_officeid=0'\n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page)+ news_domain\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "\n",
    "        for url in url_list:\n",
    "            driver = webdriver.Chrome(executable_path=driver_path) \n",
    "            try:\n",
    "                driver.get(url)\n",
    "                time.sleep(1)\n",
    "            except:\n",
    "                print('link error')\n",
    "                continue\n",
    "                \n",
    "            # try문 안에서 각자 언론사에 맞게 수정\n",
    "            try:\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(soup.select('#title_sns')[0].text)\n",
    "                data = soup.find('article',class_='viewBox').text\n",
    "                data=data.strip()\n",
    "                data=clean_text(data)\n",
    "                data_list.append(data)\n",
    "            except:\n",
    "                print(url)\n",
    "                print('None_type')\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "        \n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 아시아투데이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asiatoday(query,driver_path,page):\n",
    "    domain = '아시아투데이'\n",
    "    driver = webdriver.Chrome(executable_path=driver_path)\n",
    "    \n",
    "    current_page = 1 \n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    head_list = []\n",
    "    data_list = []\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2268&nso=&is_sug_officeid=0'\n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > sec6tion > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "\n",
    "        for url in url_list:\n",
    "            driver = webdriver.Chrome(executable_path=driver_path)  # for Windows\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                time.sleep(1)\n",
    "            except:\n",
    "                print('link error')\n",
    "                continue\n",
    "            # try문 안에서 각자 언론사에 맞게 수정\n",
    "            try:\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(soup.select('#section_top > div > h3')[0].text)\n",
    "                data = soup.find('div',class_='news_bm').text\n",
    "                data=data.strip()\n",
    "                data=clean_text(data)\n",
    "                data_list.append(data)\n",
    "            except:\n",
    "                print(url)\n",
    "                print('None_type')\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "        \n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전국매일신문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alldays(query,driver_path,page):\n",
    "    head_list = []\n",
    "    data_list = []\n",
    "    current_page = 1 \n",
    "    last_page =(int(page)-1)*10+1\n",
    "    domain = '전국매일신문'\n",
    "    \n",
    "    while current_page < last_page:       \n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options) \n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain = '&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2844&nso=&is_sug_officeid=0'\n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "        for url in url_list:\n",
    "            try:\n",
    "                driver = webdriver.Chrome(executable_path=driver_path, options=options)  # for Windows\n",
    "                driver.get(url)\n",
    "                time.sleep(2)\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head = clean_text(soup.find('div',class_=\"article-head-title\").text)\n",
    "                head_list.append(head)\n",
    "                tags = soup.select_one('#article-view-content-div').find_all('p')\n",
    "                data = [clean_text(x.text) for x in tags]\n",
    "                data_list.append(' '.join(data))\n",
    "            except:\n",
    "                print(url)\n",
    "                print(\"None_type\")\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                           '제목' :  head_list,\n",
    "                           '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 조선일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joseon(query,driver_path,page):\n",
    "    head_list = []\n",
    "    data_list = []\n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    domain = '조선일보'\n",
    "    \n",
    "    while current_page < last_page:        \n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain = '&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1023&nso=&is_sug_officeid=0'\n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "        for url in url_list:\n",
    "            try:\n",
    "                driver = webdriver.Chrome(executable_path=driver_path, options=options)\n",
    "                driver.get(url)\n",
    "                time.sleep(2)\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head = clean_text(soup.find_all(\"h1\", class_=\"article-header__headline | font--primary text--black\")[0].text)\n",
    "                head_list.append(head)\n",
    "                tags = soup.find_all('p',class_=' article-body__content article-body__content-text | text--black text font--size-sm-18 font--size-md-18 font--primary')\n",
    "                data = [clean_text(x.text) for x in tags]\n",
    "                data_list.append(' '.join(data))\n",
    "            except:\n",
    "                print(\"None_type\")\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                           '제목' :  head_list,\n",
    "                           '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 중앙일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chungang(query,driver_path,page):\n",
    "    head_list = []\n",
    "    data_list = []\n",
    "    current_page = 1 \n",
    "    last_page =(int(page)-1)*10+1\n",
    "    domain = '중앙일보'\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options)  \n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain = '&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1025&nso=&is_sug_officeid=0'\n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "        for url in url_list:\n",
    "            try:\n",
    "                driver = webdriver.Chrome(executable_path=driver_path, options=options)  # for Windows\n",
    "                driver.get(url)\n",
    "                time.sleep(2)\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head = clean_text(soup.select('#article_title')[0].text)\n",
    "                head_list.append(head)\n",
    "                text = soup.find('div', id='article_body').text\n",
    "                text = clean_text(text)\n",
    "                data_list.append(text)\n",
    "            except:\n",
    "                print(url)\n",
    "                print(\"None_type\")\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                           '제목' :  head_list,\n",
    "                           '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 천지일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newscj(query,driver_path,page):\n",
    "    head_list = []\n",
    "    data_list = []\n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    domain = '천지일보'\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options) \n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2041&nso=&is_sug_officeid=0' \n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "        for url in url_list:\n",
    "            try:\n",
    "                driver = webdriver.Chrome(executable_path=driver_path, options=options) \n",
    "                driver.get(url)\n",
    "                time.sleep(2)\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head = clean_text(soup.find('div',class_=\"article-head-title\").text)\n",
    "                head_list.append(head)\n",
    "                tags = soup.select_one('#article-view-content-div').find_all('p')\n",
    "                data = [clean_text(x.text) for x in tags]\n",
    "                data_list.append(' '.join(data))\n",
    "            except:\n",
    "                print(url)\n",
    "                print(\"None_type\")\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                           '제목' :  head_list,\n",
    "                           '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한겨례"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hani(query,driver_path,page):\n",
    "    head_list = []\n",
    "    data_list = []\n",
    "    current_page = 1 \n",
    "    last_page =(int(page)-1)*10+1\n",
    "    domain = '한겨례'\n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path=driver_path, options=options) \n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain ='&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1028&nso=&is_sug_officeid=0' \n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "        for url in url_list:\n",
    "            try:\n",
    "                driver = webdriver.Chrome(executable_path=driver_path, options=options)  \n",
    "                driver.get(url)\n",
    "                time.sleep(2)\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head = clean_text(soup.find('span', class_='title').text)\n",
    "                head_list.append(head)\n",
    "                tags = soup.select_one('#a-left-scroll-in > div.article-text > div').find_all('div', class_='text')\n",
    "                data = [clean_text(x.text) for x in tags]\n",
    "                data_list.append(' '.join(data))\n",
    "            except:\n",
    "                print(\"None_type\")\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                           '제목' :  head_list,\n",
    "                           '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한국일보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hankook(query,driver_path,page):\n",
    "    domain = '한국일보'\n",
    "    driver = webdriver.Chrome(executable_path=driver_path, options=options)  # for Windows\n",
    "    \n",
    "    current_page = 1\n",
    "    last_page =(int(page)-1)*10+1\n",
    "    \n",
    "    head_list = []\n",
    "    data_list = []\n",
    "    \n",
    "    \n",
    "    while current_page < last_page:\n",
    "        driver = webdriver.Chrome(executable_path = driver_path)\n",
    "        url_domain = 'https://search.naver.com/search.naver?where=news&query='\n",
    "        news_domain = '%EB%89%B4%EC%8A%A4&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=1469&nso=&is_sug_officeid=0'\n",
    "        url = url_domain + query +'&%2Ca%3A&start='+str(current_page) + news_domain\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "        \n",
    "        # 기사 url을 list형태로 반환\n",
    "        try:\n",
    "            news_list = soup.select('#main_pack > section > div > div.group_news > ul')\n",
    "            url_list = []\n",
    "            news_list = news_list[0].find_all('li')\n",
    "            for url in news_list:\n",
    "                url_list.append(str(url.find_all('a')[0].get('data-url')))\n",
    "        except:\n",
    "            print('기사가 없습니다.')\n",
    "            driver.close()\n",
    "            return 0\n",
    "        driver.close()\n",
    "\n",
    "        for url in url_list:\n",
    "            driver = webdriver.Chrome(executable_path=driver_path, options=options)  # for Windows\n",
    "            driver.get(url)\n",
    "            time.sleep(2)\n",
    "            # try문 안에서 각자 언론사에 맞게 수정\n",
    "            try:\n",
    "                soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "                head_list.append(clean_text(soup.find('h2').text))\n",
    "                tags = soup.select_one('body > div.wrap > div.container.end.end-uni > div.end-body > div > div.col-main.read').find_all('p')\n",
    "                data = [clean_text(x.text) for x in tags]\n",
    "                data_list.append(data)\n",
    "            except:\n",
    "                print(url)\n",
    "                print('None_type')\n",
    "            driver.close()\n",
    "        current_page+=10\n",
    "\n",
    "    output = pd.DataFrame({'언론사' : [domain]*len(head_list),\n",
    "                          '제목' :  head_list,\n",
    "                          '내용' : data_list})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 원하는 keyword와 페이지 수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '\"풀무원\" 바른먹거리'\n",
    "max_page=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=101&oid=032&aid=0000069224\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=032&aid=0000070370\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=032&aid=0000070370\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=032&aid=0000070370\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=032&aid=0000070370\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=032&aid=0000070370\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=032&aid=0000070370\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=032&aid=0000070370\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=032&aid=0000070370\n",
      "None_type\n"
     ]
    }
   ],
   "source": [
    "biz_df = biz_khan(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'지난 16일 사회관계망서비스에서는 차밑에 들어가 있는 한 노동자의 사진이 화제를 모았다. ‘바른 먹거리’로 유명한 식품기업 풀무원의 물품을 운송하는 화물노동자 이현철씨가 충청북도 음성 풀무원 물류센터에서 쇠사슬을 몸에 감고 농성하는 모습이었다. 민주노총 충북지역본부 페이스북은 이 사진을 게시하고 “풀무원 분회 이현철 동지가 본인의 차량으로 물류센타 정문으로 돌진하여 스스로 쇠사슬을 몸에 감았다”며 “이현철 동지는 풀무원 제품을 하차중에 개방성 골절살이 찢어져 뼈가 보이고 부러진 상태이 되어도 회사의 대차비 요구에 완전히 회복하지 못하고 하루에 700를 운행하며 상하차 업무까지 했던 동지”라고 소개했다. 또 “풀무원의 거의 모든 동지들이 이렇게 지내고 있다”며 “풀무원의 추악함과 가증스러움 때문에 분노가 치밀어 미치겠다”고도 적었다. 지난 16일 이현철씨가 풀무원 물류센타에 세워둔 차 밑에 쇠사슬을 감고 누워있다. 민주노총 충북지역본부 페이스북 풀무원은 ‘바른먹거리’란 슬로건을 내건 식품회사다. 현재는 경영권에서 손을 뗐지만 새정치민주연합 원혜영 의원이 창업한 기업으로도 잘 알려져있다. 그러나 최근에는 상에서 “‘바른먹거리’ 뒤에는 ‘나쁜짓거리’”라는 비아냥을 받고 있다. 화물연대와의 대립 때문이다. 시작은 지난 4일이었다. 연합뉴스에 따르면 풀무원의 충북 음성 물류사업장에서 화물업체 운송트럭 지입차주 40여명이 이날부터 파업에 돌입했다. 이들은 풀무원의 물류계열사 엑소후레쉬물류의 위탁업체인 대원냉동운수·서울가람물류와 계약을 맺고 용역트럭5 11을 모는 노동자들이었다. 파업의 주된 이유는 ‘트럭 외부 도색과 스티커’였다. 지입차주들은 “회사는 지난 파업 때 차량 도색 훼손을 문제 삼아 차량 도색을 훼손하지 않을 것과 구호 주장 화물연대 스티커 등을 부착하지 않을 것을 요구하고 이를 어기면 징벌적 임금 삭감을 하겠다는 노예 계약서를 강요하고 있다”며 “사측은 노예 계약서인 ‘도색 유지 계약서’를 즉각 폐기하고 산재 사고가 발생했을 때 보상하는 방안을 마련하라”고 말했다. 화물연대는 지난 3일 동영상 사이트 ‘유튜브’에 풀무원을 다각도로 비판하는 동영상을 게재했다. 지난 8일에는 영화 ‘베테랑’의 내용을 섞어 화물노동자들의 어려움 부당한 처우 등을 전하는 동영상 2탄도 유튜브에 걸었다. 풀무원 측은 화물연대 측 주장을 조목조목 반박하고 나섰다. 풀무원은 지난 8일 발표한 보도자료에 “풀무원 브랜드 로고를 훼손하지 않기로 한 서약서는 차주들이 자발적으로 작성한 것”이라며 “풀무원 를 도색 했을 때가 하지 않았을 때보다 차량 거래시 수천만 원의 프리미엄이 붙기 때문”이라고 말했다. 또 “지난 1월 상호 협력과 상생을 위해 앞으로 1년간 일방적인 제품 운송 거부 등 집단행동을 하지 않기로 합의했음에도 차주들이 명분 없는 불법 상황을 또 연출하고 있다”고 주장했다. 14일에 낸 보도자료에서는 “문제가 되는회사 없이 차라리 백지로 차량을 운행하라”고 요구했다. 풀무원은 “도색유지서약서가 나온 배경은 지난 1월 화물연대 파업때 가 도색된 차량 외부가 심하게 훼손됐기 때문”이라며 “풀무원 가 도색된 차량을 다시 훼손할 바에는 차량을 백지로 칠하는 것이 낫다. 회사는 도색을 지울 경우 도색비용까지 지급하겠다”고 말했다. 또 “운송거부중인 지입차주 40여명은 서약서 폐기를 주장하면서도 풀무원의 는 지우지 않겠다는 입장”이라며 “풀무원 를 지우면 차량을 매매할 때 가치로 받을 수 있는 수천만 원의 프리미엄권리금을 포기해야 하기 때문”이라고 주장했다. 풀무원측이 공개한 도색 훼손 차량 사진 풀무원은 또 운송거부중인 지입차주들이 합의내용과 관계도 없는 허위주장을 담은 동영상을 만들어 유포해 기업이미지와 명예를 훼손하고 막대한 경제적 손실을 입히고 있다고 주장했다. 지입차주들이 ‘20년 동안 월급이 동결되었고 추가 운임비는 줄고 인력감축으로 노동강도는 세졌다’고 동영상을 통해 주장하는 것에 대해 회사는 “이들이 받는 돈은 월급이 아니라 제품을 운송해주고 운송회사에서 받는 운임인데 이는 꾸준히 올랐다”고 말했다. 이어 “운송회사에서 지입차주들에게 지급하는 한달 평균 기본운임은 512만원이며 추가 운임비를 포함하면 평균 600만 원 수준이다”라며 “지입차주들은 연 7000만원 안팎의 안정적인 수입을 올리는 개인사업자다”라고 말했다. 화물연대 심동진 전략조직국장은 17일 경향신문과 통화에서 “풀무원 사측이 아주 조그만 스티커 하나까지 문제를 삼기 때문에 벌어진 일”이라고 말했다. 심 국장은 “회사가 강요하는 계약서대로라면 단체행동은커녕 화물연대 소속임을 표시하는 작은 스티커 깃발도 차량에 달 수가 없다”며 “이를 문제삼아 벌금을 물리고 계약해지를 주장하는 것은 사실상의 노예계약”이라고 말했다. 또 “우리가 주장하는 것은 이런 노예계약을 무효화하자는 것인데 회사 측이 협상에도 나오지 않아 답답할 따름”이라고 말했다.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biz_df['내용'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://news.kukinews.com/article/view.asp?arcid=0008618427&code=41171811&cp=nv\n",
      "None_type\n",
      "http://news.kukinews.com/article/view.asp?arcid=0008613476&code=41171811&cp=nv\n",
      "None_type\n",
      "http://news.kukinews.com/article/view.asp?arcid=0008568919&code=41171811&cp=nv\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=101&oid=005&aid=0000166450\n",
      "None_type\n",
      "http://news.kukinews.com/article/view.asp?page=1&gCode=cul&arcid=0007343144&cp=nv\n",
      "None_type\n",
      "http://news.kmib.co.kr/article/view.asp?arcid=0008453804&code=41171811&cp=nv\n",
      "None_type\n",
      "http://www.kukinews.com/news2/article/view.asp?page=1&gCode=all&arcid=1237210098&cp=nv\n",
      "None_type\n"
     ]
    }
   ],
   "source": [
    "kmib_df = kmib(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=101&oid=086&aid=0000048442\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=101&oid=086&aid=0002128522\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=086&aid=0002161049\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=101&oid=086&aid=0002168403\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=086&aid=0002152391\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=101&oid=086&aid=0002078723\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=101&oid=086&aid=0001957511\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=101&oid=086&aid=0001938848\n",
      "None_type\n"
     ]
    }
   ],
   "source": [
    "naeil_df = naeil(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "donga_df = donga(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://search.naver.com/search.naver?where=news&query=\"풀무원\" 바른먹거리&%2Ca%3A&start=1&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2385&nso=so%3Ar%2Cp%3Aall%2Ca%3Aall&is_sug_officeid=0\n",
      "https://search.naver.com/search.naver?where=news&query=\"풀무원\" 바른먹거리&%2Ca%3A&start=11&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2385&nso=so%3Ar%2Cp%3Aall%2Ca%3Aall&is_sug_officeid=0\n",
      "https://search.naver.com/search.naver?where=news&query=\"풀무원\" 바른먹거리&%2Ca%3A&start=21&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2385&nso=so%3Ar%2Cp%3Aall%2Ca%3Aall&is_sug_officeid=0\n",
      "https://search.naver.com/search.naver?where=news&query=\"풀무원\" 바른먹거리&%2Ca%3A&start=31&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2385&nso=so%3Ar%2Cp%3Aall%2Ca%3Aall&is_sug_officeid=0\n",
      "https://search.naver.com/search.naver?where=news&query=\"풀무원\" 바른먹거리&%2Ca%3A&start=41&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2385&nso=so%3Ar%2Cp%3Aall%2Ca%3Aall&is_sug_officeid=0\n",
      "https://search.naver.com/search.naver?where=news&query=\"풀무원\" 바른먹거리&%2Ca%3A&start=51&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2385&nso=so%3Ar%2Cp%3Aall%2Ca%3Aall&is_sug_officeid=0\n",
      "https://search.naver.com/search.naver?where=news&query=\"풀무원\" 바른먹거리&%2Ca%3A&start=61&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2385&nso=so%3Ar%2Cp%3Aall%2Ca%3Aall&is_sug_officeid=0\n",
      "https://search.naver.com/search.naver?where=news&query=\"풀무원\" 바른먹거리&%2Ca%3A&start=71&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2385&nso=so%3Ar%2Cp%3Aall%2Ca%3Aall&is_sug_officeid=0\n",
      "https://search.naver.com/search.naver?where=news&query=\"풀무원\" 바른먹거리&%2Ca%3A&start=81&sm=tab_opt&sort=0&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=1&office_type=1&office_section_code=1&news_office_checked=2385&nso=so%3Ar%2Cp%3Aall%2Ca%3Aall&is_sug_officeid=0\n"
     ]
    }
   ],
   "source": [
    "mi_df = m_i(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "munhwa_df = munhwa(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://biz.seoul.co.kr/news/newsView.php?id=20210315500025&wlog_tag3=naver\n",
      "None_type\n",
      "https://www.seoul.co.kr/news/newsView.php?id=20200612014002&wlog_tag3=naver\n",
      "None_type\n",
      "http://nownews.seoul.co.kr/news/newsView.php?id=20180530601018&wlog_tag3=naver\n",
      "None_type\n",
      "http://nownews.seoul.co.kr/news/newsView.php?id=20180809601005&wlog_tag3=naver\n",
      "None_type\n",
      "http://nownews.seoul.co.kr/news/newsView.php?id=20160414601010\n",
      "None_type\n",
      "http://nownews.seoul.co.kr/news/newsView.php?id=20170612601013&wlog_tag3=naver\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=103&oid=081&aid=0000082014\n",
      "None_type\n",
      "http://nownews.seoul.co.kr/news/newsView.php?id=20100903603077\n",
      "None_type\n"
     ]
    }
   ],
   "source": [
    "seoul_df = seoul(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "link error\n",
      "http://www.economysegye.com/articles/view.html?aid=20130326001515&cid=7111000000000&OutUrl=naver\n",
      "None_type\n",
      "link error\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=022&aid=0000092234\n",
      "None_type\n",
      "https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=022&aid=0000092149\n",
      "None_type\n"
     ]
    }
   ],
   "source": [
    "segye_df = segye(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기사가 없습니다.\n"
     ]
    }
   ],
   "source": [
    "asia_df = asiatoday(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기사가 없습니다.\n"
     ]
    }
   ],
   "source": [
    "alldays_df = alldays(query, driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "joseon_df = joseon(query, driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기사가 없습니다.\n"
     ]
    }
   ],
   "source": [
    "chungang_df = chungang(query, driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "cj_df = newscj(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None_type\n",
      "None_type\n"
     ]
    }
   ],
   "source": [
    "hani_df = hani(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "hk_df = hankook(query,driver_path,max_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 네이버 뉴스 모든 일간지 이름 list (dataframe 합칠때 None인 데이터들은 구별해주기 위한 작업)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list=[biz_df, kmib_df, naeil_df, donga_df, mi_df, munhwa_df, seoul_df, segye_df, asia_df, alldays_df ,joseon_df, chungang_df, cj_df, hani_df, hk_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=[]\n",
    "for name in df_list:\n",
    "    if type(name)!=int:\n",
    "        names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df=pd.concat(names)\n",
    "news_df.drop_duplicates(['제목'],inplace=True,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>언론사</th>\n",
       "      <th>제목</th>\n",
       "      <th>내용</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>경향신문</td>\n",
       "      <td>‘바른 먹거리’ 풀무원 물류센터에서는 무슨 일이?</td>\n",
       "      <td>지난 16일 사회관계망서비스에서는 차밑에 들어가 있는 한 노동자의 사진이 화제를 모...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>경향신문</td>\n",
       "      <td>어린이 ‘바른 먹거리 캠페인’ 신청하세요… 풀무원, 사회공헌 교육 확대</td>\n",
       "      <td>풀무원이 어린이들의 바른 식생활을 위해 2010년부터 실시해온 ‘바른 먹거리 캠페인...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>경향신문</td>\n",
       "      <td>풀무원, 어린이 바른 먹거리 현장 체험교육</td>\n",
       "      <td>풀무원 홀딩스는 초등학교 여름방학을 맞아 충북 음성 풀무원 두부공장을 돌아보는 ‘풀...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>경향신문</td>\n",
       "      <td>풀무원 ‘바른 먹을거리 교육’에 어린이 신청 늘어</td>\n",
       "      <td>어린이를 대상으로 한 올바른 먹을거리 교육이 학교와 학부모들로부터 관심을 모으고 있...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>경향신문</td>\n",
       "      <td>풀무원, 13년 만에 새 CI 발표... 글로벌 로하스 도약 목표</td>\n",
       "      <td>풀무원 새 풀무원이 2005년 이후 13년 만에 기업이미지를 교체했다. 계열사 브랜...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>한겨례</td>\n",
       "      <td>“일 25씩 더 해 기부하면 세계 굶주림 사라져”</td>\n",
       "      <td>“세상의 아름다움 중에서 인간의 아름다움을 딛고 넘어설 만한 아름다움은 없다고 생각...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>한국일보</td>\n",
       "      <td>냉면·짜장·떡볶이 제품에 트레이가 필요한가요</td>\n",
       "      <td>[쓰레기를 사지 않을 권리15즉석조리식품 트레이, 기후위기와 쓰레기산에 신음하면서도...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>한국일보</td>\n",
       "      <td>풀무원 입사 1호 사원이 … 성장세 발목 잡던 해외 실적도 개선</td>\n",
       "      <td>[원혜영 의원 부친 농장 남승우 前대표가 기업화, ‘바른 먹거리’로 年 매출 2조 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>한국일보</td>\n",
       "      <td>풀무원 13년 만에 기업이미지 바꿔… 계열사 브랜드도 통일</td>\n",
       "      <td>[풀무원이 2005년 이후 13년 만에 기업이미지를 교체하고 계열사 브랜드 체계를 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>한국일보</td>\n",
       "      <td>위원회 출범하고 도…식품업계 경영 속도</td>\n",
       "      <td>[기업의 비非재무적 성과를 뜻하는 환경·사회·지배구조 경영이 유통업계의 주요한 생존...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>254 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      언론사                                       제목  \\\n",
       "0    경향신문              ‘바른 먹거리’ 풀무원 물류센터에서는 무슨 일이?   \n",
       "1    경향신문  어린이 ‘바른 먹거리 캠페인’ 신청하세요… 풀무원, 사회공헌 교육 확대   \n",
       "2    경향신문                  풀무원, 어린이 바른 먹거리 현장 체험교육   \n",
       "3    경향신문              풀무원 ‘바른 먹을거리 교육’에 어린이 신청 늘어   \n",
       "4    경향신문     풀무원, 13년 만에 새 CI 발표... 글로벌 로하스 도약 목표   \n",
       "..    ...                                      ...   \n",
       "249   한겨례              “일 25씩 더 해 기부하면 세계 굶주림 사라져”   \n",
       "250  한국일보                 냉면·짜장·떡볶이 제품에 트레이가 필요한가요   \n",
       "251  한국일보      풀무원 입사 1호 사원이 … 성장세 발목 잡던 해외 실적도 개선   \n",
       "252  한국일보         풀무원 13년 만에 기업이미지 바꿔… 계열사 브랜드도 통일   \n",
       "253  한국일보                    위원회 출범하고 도…식품업계 경영 속도   \n",
       "\n",
       "                                                    내용  \n",
       "0    지난 16일 사회관계망서비스에서는 차밑에 들어가 있는 한 노동자의 사진이 화제를 모...  \n",
       "1    풀무원이 어린이들의 바른 식생활을 위해 2010년부터 실시해온 ‘바른 먹거리 캠페인...  \n",
       "2    풀무원 홀딩스는 초등학교 여름방학을 맞아 충북 음성 풀무원 두부공장을 돌아보는 ‘풀...  \n",
       "3    어린이를 대상으로 한 올바른 먹을거리 교육이 학교와 학부모들로부터 관심을 모으고 있...  \n",
       "4    풀무원 새 풀무원이 2005년 이후 13년 만에 기업이미지를 교체했다. 계열사 브랜...  \n",
       "..                                                 ...  \n",
       "249  “세상의 아름다움 중에서 인간의 아름다움을 딛고 넘어설 만한 아름다움은 없다고 생각...  \n",
       "250  [쓰레기를 사지 않을 권리15즉석조리식품 트레이, 기후위기와 쓰레기산에 신음하면서도...  \n",
       "251  [원혜영 의원 부친 농장 남승우 前대표가 기업화, ‘바른 먹거리’로 年 매출 2조 ...  \n",
       "252  [풀무원이 2005년 이후 13년 만에 기업이미지를 교체하고 계열사 브랜드 체계를 ...  \n",
       "253  [기업의 비非재무적 성과를 뜻하는 환경·사회·지배구조 경영이 유통업계의 주요한 생존...  \n",
       "\n",
       "[254 rows x 3 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.to_csv('풀무원 바른먹거리.csv', encoding='UTF-8')\n",
    "test = pd.read_csv('풀무원 바른먹거리.csv',encoding='UTF-8',index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>언론사</th>\n",
       "      <th>제목</th>\n",
       "      <th>내용</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>경향신문</td>\n",
       "      <td>‘바른 먹거리’ 풀무원 물류센터에서는 무슨 일이?</td>\n",
       "      <td>지난 16일 사회관계망서비스에서는 차밑에 들어가 있는 한 노동자의 사진이 화제를 모...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>경향신문</td>\n",
       "      <td>어린이 ‘바른 먹거리 캠페인’ 신청하세요… 풀무원, 사회공헌 교육 확대</td>\n",
       "      <td>풀무원이 어린이들의 바른 식생활을 위해 2010년부터 실시해온 ‘바른 먹거리 캠페인...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>경향신문</td>\n",
       "      <td>풀무원, 어린이 바른 먹거리 현장 체험교육</td>\n",
       "      <td>풀무원 홀딩스는 초등학교 여름방학을 맞아 충북 음성 풀무원 두부공장을 돌아보는 ‘풀...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>경향신문</td>\n",
       "      <td>풀무원 ‘바른 먹을거리 교육’에 어린이 신청 늘어</td>\n",
       "      <td>어린이를 대상으로 한 올바른 먹을거리 교육이 학교와 학부모들로부터 관심을 모으고 있...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>경향신문</td>\n",
       "      <td>풀무원, 13년 만에 새 CI 발표... 글로벌 로하스 도약 목표</td>\n",
       "      <td>풀무원 새 풀무원이 2005년 이후 13년 만에 기업이미지를 교체했다. 계열사 브랜...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>한겨례</td>\n",
       "      <td>“일 25씩 더 해 기부하면 세계 굶주림 사라져”</td>\n",
       "      <td>“세상의 아름다움 중에서 인간의 아름다움을 딛고 넘어설 만한 아름다움은 없다고 생각...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>한국일보</td>\n",
       "      <td>냉면·짜장·떡볶이 제품에 트레이가 필요한가요</td>\n",
       "      <td>['쓰레기를 사지 않을 권리15즉석조리식품 트레이', '기후위기와 쓰레기산에 신음하...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>한국일보</td>\n",
       "      <td>풀무원 입사 1호 사원이 … 성장세 발목 잡던 해외 실적도 개선</td>\n",
       "      <td>['원혜영 의원 부친 농장 남승우 前대표가 기업화', '‘바른 먹거리’로 年 매출 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>한국일보</td>\n",
       "      <td>풀무원 13년 만에 기업이미지 바꿔… 계열사 브랜드도 통일</td>\n",
       "      <td>['풀무원이 2005년 이후 13년 만에 기업이미지를 교체하고 계열사 브랜드 체계를...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>한국일보</td>\n",
       "      <td>위원회 출범하고 도…식품업계 경영 속도</td>\n",
       "      <td>['기업의 비非재무적 성과를 뜻하는 환경·사회·지배구조 경영이 유통업계의 주요한 생...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>254 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      언론사                                       제목  \\\n",
       "0    경향신문              ‘바른 먹거리’ 풀무원 물류센터에서는 무슨 일이?   \n",
       "1    경향신문  어린이 ‘바른 먹거리 캠페인’ 신청하세요… 풀무원, 사회공헌 교육 확대   \n",
       "2    경향신문                  풀무원, 어린이 바른 먹거리 현장 체험교육   \n",
       "3    경향신문              풀무원 ‘바른 먹을거리 교육’에 어린이 신청 늘어   \n",
       "4    경향신문     풀무원, 13년 만에 새 CI 발표... 글로벌 로하스 도약 목표   \n",
       "..    ...                                      ...   \n",
       "249   한겨례              “일 25씩 더 해 기부하면 세계 굶주림 사라져”   \n",
       "250  한국일보                 냉면·짜장·떡볶이 제품에 트레이가 필요한가요   \n",
       "251  한국일보      풀무원 입사 1호 사원이 … 성장세 발목 잡던 해외 실적도 개선   \n",
       "252  한국일보         풀무원 13년 만에 기업이미지 바꿔… 계열사 브랜드도 통일   \n",
       "253  한국일보                    위원회 출범하고 도…식품업계 경영 속도   \n",
       "\n",
       "                                                    내용  \n",
       "0    지난 16일 사회관계망서비스에서는 차밑에 들어가 있는 한 노동자의 사진이 화제를 모...  \n",
       "1    풀무원이 어린이들의 바른 식생활을 위해 2010년부터 실시해온 ‘바른 먹거리 캠페인...  \n",
       "2    풀무원 홀딩스는 초등학교 여름방학을 맞아 충북 음성 풀무원 두부공장을 돌아보는 ‘풀...  \n",
       "3    어린이를 대상으로 한 올바른 먹을거리 교육이 학교와 학부모들로부터 관심을 모으고 있...  \n",
       "4    풀무원 새 풀무원이 2005년 이후 13년 만에 기업이미지를 교체했다. 계열사 브랜...  \n",
       "..                                                 ...  \n",
       "249  “세상의 아름다움 중에서 인간의 아름다움을 딛고 넘어설 만한 아름다움은 없다고 생각...  \n",
       "250  ['쓰레기를 사지 않을 권리15즉석조리식품 트레이', '기후위기와 쓰레기산에 신음하...  \n",
       "251  ['원혜영 의원 부친 농장 남승우 前대표가 기업화', '‘바른 먹거리’로 年 매출 ...  \n",
       "252  ['풀무원이 2005년 이후 13년 만에 기업이미지를 교체하고 계열사 브랜드 체계를...  \n",
       "253  ['기업의 비非재무적 성과를 뜻하는 환경·사회·지배구조 경영이 유통업계의 주요한 생...  \n",
       "\n",
       "[254 rows x 3 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
